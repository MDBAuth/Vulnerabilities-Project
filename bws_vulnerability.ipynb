{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BWS Vulnerabilities Project\n",
    "\n",
    "### Project Team\n",
    "* Jennifer Hale (lead)\n",
    "* Cherie Campbell (vegetation)\n",
    "* Heather McGinness (waterbirds)\n",
    "* Shane Brooks (spatial analytics lead contact for an questions on this notebook shane@brooks.eco )\n",
    "* Enzo Guarino (spatial analytics)\n",
    "\n",
    "### Client Managers\n",
    "* Luke Taylor (CEWO)\n",
    "* Alex Meehan (MDBA)\n",
    "\n",
    "## Summary\n",
    "The BWS Priorities Project aimed to spatially and temporally summarise metrics of vulnerability (combining condition and stress) for vegetation and waterbirds in the Murray-Darling Basin with the aim of informing the setting of annual watering priorities for these target groups.\n",
    "\n",
    "**Project report**: Hale, J., Brooks, S., Campbell, C. and McGinness, H. (2023) Assessing Vulnerability for use in Determining Basin-scale Environmental Watering Priorities. A Report to the Commonwealth Environmental Water Office, Canberra.\n",
    "\n",
    "This Jupyter notebook is the final stage of data processing that pulls together multiple data sets to summarise and score the condition metrics, stress metrics and then add the scores to the final vulnerability metric.  Multiple input data files are read in, pivoted to tabular format with years as columns.  The measurement of vulnerability relies on first calculating the long-term baseline (mean of all years excluding the millennium drought) then scoring the deviation from the baseline.   Metrics calculated for ANAE ecosystem polygons are aggregated together as an area weighted average for larger spatial units (e.g. Ramsar sites, valleys).\n",
    "\n",
    "\n",
    "\n",
    "## Data Inputs\n",
    " 1. Australian National Aquatic Ecosystem (ANAE) mapping  v3 - The ANAE identifies different vegetation types and provides the spatial units used to summarise other data. Polygons < 1 Ha are removed as they are too small to meet the reliability requirements of the WIT tool and MODIS derived NDVI.  \n",
    " 1. Geosciences Australia Wetland Insights Tool (WIT) - WIT data observations for all ANAE polygons > 1 Ha in the MDB 1986-present.  Raw data supplied by Geosciences Australia for individual observation dates through the Landsat Record summarised into daily, yearly, all-time and inundation event statistics (a separate jupyter notebook)\n",
    " 1. MDBA Tree Stand Condition Tool Rasters - Average condition per ANAE polygon per year 1986-present calculated using google earth engine reducer shared code: https://code.earthengine.google.com/3b7223339c2c3cffb973b1280d5dd047 or https://code.earthengine.google.com/?scriptPath=users%2Flitepc%2FShaneGitRepo%3AeeBWS_MeanTSC_ANAEv3\n",
    " 1. Normalized Difference Vegetation Index (NDVI) - Average NDVI per ANAE polygon per year 1986-present calculated using google earth engine reducer: shared code: https://code.earthengine.google.com/4db0904f68e5c27fef8e8eb8fb75a375  or  https://code.earthengine.google.com/?scriptPath=users%2Flitepc%2FShaneGitRepo%3AeeBWS_MODIS_NDVI_ANAEv3\n",
    " 1. Root Zone Soil Moisture (Australian Water Outlook) - Mean root zone soil moisture per ANAE polygon per year was generated using ArcGIS but there are many ways to calculate the annual average per polygon from the AWO netcdf   https://awo.bom.gov.au/products/historical/soilMoisture-rootZone\n",
    " 1. Aggregated Waterbird observations (Atlas of Living Australia combined with Aerial Surveys) - Data set cleaned and manually curated by Jennifer Hale with input from Heather McGuinness\n",
    " 1. Stress thresholds for vegetation and waterbirds based on durations since last inundation for different functional groups that were identified by experts are coded directly into this Jupyter Notebook\n",
    " \n",
    "## Data Outputs\n",
    "This notebook writes the various metric to the working directory in tabular format csv files (spatial units in rows, years in columns) that can be read by Microsoft Excel.  Baseline values and scores are added to the tables as additional columns. There are a **lot** of output files included for spatial scales that were not included in the project report but may be useful for other investigations or to inform water planning at those locations (e.g. DIWA and Ramsar sites)\n",
    "\n",
    "Output files for habitat metrics follow the naming convention: {metric}_{aggregator}_{year_window_width}yr_condition.csv\n",
    "e.g.  pv_median_DIWA_5yr_condition.csv  is the median \"pv\" (green fractional cover) with ANAE polygons aggregated to larger DIWA wetland scales using a 5-year moving window in which to calculate rates of change.  Output files for waterbirds also use codes (sr = species richness, g = functional groups, i = counts of individuals).\n",
    "\n",
    "*NOTE:  The outputs generated from this notebook do vary slightly from those in the report because this code was finalised after the report was written. Waterbirds tables were constructed in Excel for the report and there are some rounding differences and formulae improvements to the way scores are normalised in the Python code. The patterns and overall outputs are consistent.\n",
    "\n",
    "### The output files that align with the main tables in the report are\n",
    "\n",
    "* Vegetation **FINAL_BWSVulnerability_vegetation_Valley.csv**\n",
    "\n",
    "* Waterbirds **FINAL_BWSVulnerability_waterbirds_Basin.csv**\n",
    "\n",
    "\n",
    "### Mapping the outputs\n",
    "\n",
    "* Patterns can be visualised in GIS by joining the output files to the relevant spatial layers.  Many of the vegetation maps in the report used the ANAE polygons scale to visualise the patterns - this was done by joining **FINAL_BWSVulnerability_vegetation_ANAE.csv** to the **ANAEv3** using the **UID** polygon identifier.  Mapping whole Valley aggregated scores would be done by joining **FINAL_BWSVulnerability_vegetation_Valley.csv** to **BWSRegions.shp** using the **BWS_Region**.\n",
    "\n",
    "\n",
    "## Processing Environment\n",
    "For the project the analysis was conducted in the python processing environment of ArcGIS Pro 3.0 but were coded to use common open source python data processing libraries (Geopandas, Pandas, numpy) that should enable the analysis to be repeated in most environments.\n",
    "Note the code produces many more tables of output data than are required because it calculates parameters for multiple spatial scales.  The BWS Vulnerability Project reports mainly on vegetation outcomes at the scale of MDB Valleys (BWS Vegetation Regions), and the waterbirds are at the Basin scale.  Outputs are also generated for Ramsar sites, MDBA Waterbird Areas (the \"dirty thirty\" polygons), and individual ANAE polygons.\n",
    "\n",
    "\n",
    "## Repeating or extending the analysis to additional years of data\n",
    "Execuing the notebook on the original source data will reproduce all outputs for 1987-2021 (1986 had some incomplete data and was dropped from the reporting).\n",
    "\n",
    "Extending the analysis requires:\n",
    "1. collating new data and appending to the current 1986-2022 source files\n",
    "1. Map new waterbird points to the spatial units (toggle **join_wetlands = False** to **True** in the code below)\n",
    "1. edit the definition of the **alltime** variable to extend past 2022.\n",
    "1. re-run the notebook\n",
    "\n",
    "Source data comes from a variety of places and requires a different technologies to assemble as outlined above.  The current source files should be used as the template to append to,  which should ensure the updated files will run with this workbook.  There is some additional code built into the workbook to re-build spatial relationships among data (e.g. to map waterbird observations to ANAE polygons and the code should accommodate additional spatial units (aggregators).\n",
    "\n",
    "The code was built to test the method within the confines of a project so it isn't always pretty.    If the logic is not clear please refer to the report and reach out to the report authors with questions.\n",
    "\n",
    "***\n",
    "     \n",
    "# Contact\n",
    "- Dr Shane Brooks\n",
    "- shane@brooks.eco\n",
    "- https://brooks.eco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages\n",
    "Import Python packages that are used for the analysis.\n",
    "\n",
    "Use standard import commands; some are shown below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GDAL_DATA'] = 'C:/Program Files/ArcGIS/Pro/Resources/pedata/gdaldata'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from tqdm.notebook import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# User Defined Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the path to the spatial data (shape files)\n",
    "spatial_path = 'D:/BWSVulnerability/spatial'\n",
    "\n",
    "\n",
    "#set the path to the data input files\n",
    "data_path = 'D:/BWSVulnerability/input'\n",
    "\n",
    "#  set the working directory\n",
    "working_directory = 'D:/BWSVulnerability/output'\n",
    "\n",
    "\n",
    "# change to the user specified working directory so the worker gets written to the correct location\n",
    "os.chdir(working_directory)\n",
    "cwdpath = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise data stores  (names are data frames used in the code)\n",
    "# this allows cells in the notebook to be re-run quickly without re-reading all the iput data multiple times when we dont have to\n",
    "\n",
    "ANAE = None\n",
    "DTwaterbirds = None\n",
    "DIWA = None\n",
    "Valley = None\n",
    "Ramsar = None\n",
    "wb = None\n",
    "\n",
    "wit_yearly = None\n",
    "tsli_master = None\n",
    "idf_master = None\n",
    "ndvi = None\n",
    "soilmoisture_df = None\n",
    "TSC_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define year ranges used by the method\n",
    "\n",
    "alltime = list(range(1987, 2022)) #2021 was the last year with complete data  1986 is excluded as it has incomplete data\n",
    "millennium_drought = list(range(2001, 2010))  #excludes 2010 because last year in a range is not included\n",
    "#yearcols is a dictionary to translate numberic year column totals of pivot tables into strings as required for shapefiles and csv headers\n",
    "yearcols = {y: 'y'+str(y) for y in alltime}\n",
    "\n",
    "#VEGETATION\n",
    "veg_window_width = 5  # veg condition/stress is averaged over a moving 5 year window up to a given year\n",
    "veg_trend_width = 2   # veg metric trends are measured in the most recent 2 years leading up to a given year\n",
    "\n",
    "\n",
    "\n",
    "#Vegetation thresholds from Cherie Campbel noting that some ranges are imprecise and have gaps.  \n",
    "\n",
    "#the three bins are defined in python using three numbers [0. threshold#1, threshold2]\n",
    "\n",
    "#and scored as:  0 > LOW > threshold#1 > MEDIUM > threshold#2 > HIGH \n",
    "\n",
    "\n",
    "#Vegetation stress thresholds based on the time-since-last-inundation (tsli)\n",
    "#Threshold scores are set for three bins (LOW, MEDIUM and HIGH stress)\n",
    "vegetation_tsli_stress_thresholds = {\n",
    "    'river red gum swamps and forests': [0, 730, 1825], # RRG swamps forests and woodlands 1-2 years, 3-4 years, ≥ 5 years\n",
    "    'river red gum woodland': [0, 730, 1825], # RRG swamps forests and woodlands 1-2 years, 3-4 years, ≥ 5 years\n",
    "    'black box': [0, 1460, 2555], # Black box, 3 – 4 years, 5 – 6 years, ≥ 7 years\n",
    "    'coolibah': [0, 3650, 7300], # Coolibah, 10 years, 20 years, > 20 years\n",
    "    'lignum': [0, 1095, 1825], # Lignum, 3 years, 4 years, ≥ 5 years\n",
    "    'submerged lake': [0, 90, 120], # Submerged vegetation, < 3 months, 3 – 4 months, > 4 months\n",
    "    'tall reed beds': [0, 365, 730 ], # Tall reeds, < 1 year, 1 – 2 years, > 2 years\n",
    "    'grassy meadows': [0, 240, 300], # Grassy meadows, < 8 months, 8 – 10 months, > 10 months\n",
    "    'herbfield': [0, 365, 1460], # Herb fields, 1 year, 2 – 4 years, > 4 years\n",
    "    'clay pan': [0, 3650, 7300] #not vegetated but used for waterbirds  10 years, 20 years, > 20 years\n",
    "}\n",
    "\n",
    "# Waterbird stress metric based on the time-since-last-inundation (tsli) of wetland vegetation the waterbirds depend on\n",
    "# Threshold scores are set for three bins (LOW, MEDIUM and HIGH stress)\n",
    "# the alignment of waterbirds to habitat types is explained in the roport and is defined in the dictionary below waterbird_dom_habitats\n",
    "waterbird_habitat_stress_thresholds = {\n",
    "     'river red gum swamps and forests': [0, 365, 1460], # <1 year, 1-4 years, ≥ 5 years\n",
    "     'river red gum woodland': [0, 365, 1460], # <1 year, 1-4 years, ≥ 5 years\n",
    "     'black box': [0, 365, 1460], # <1 year, 1-4 years, ≥ 5 years\n",
    "     'coolibah': [0, 365, 1460], # <1 year, 1-4 years, ≥ 5 years\n",
    "     'lignum': [0, 365, 1460], # <1 year, 1-4 years, ≥ 5 years\n",
    "     'submerged lake': [0, 365, 1095], # <1 year, 1-3 years, ≥ 3 years\n",
    "     'tall reed beds': [0, 365, 1460], # <1 year, 1-4 years, ≥ 5 years\n",
    "     'grassy meadows': [0, 365, 1460], # <1 year, 1-4 years, ≥ 5 years\n",
    "     'herbfield': [0, 365, 1460], # <1 year, 1-4 years, ≥ 5 years\n",
    "     'clay pan': [0, 365, 1460], # <1 year, 1-4 years, ≥ 5 years\n",
    "}\n",
    "\n",
    "# Define a dictionary that identifies the importnat dominant ecosytem type groups for different\n",
    "# waterbird functional groups. This is used to take vegetation/habitat metrics to waterbird groups\n",
    "    \n",
    "waterbird_dom_habitats = {\n",
    "    'Aerial_divers': ['submerged lake'],\n",
    "    'Colonial_nesters':['river red gum swamps and forests'],\n",
    "    'Cryptic_waders': ['herbfield', 'tall reed beds'],\n",
    "    'Diving_swimmers': ['river red gum swamps and forests'],\n",
    "    'Filtering': ['river red gum swamps and forests'],\n",
    "    'Grazing_swimmers': ['river red gum swamps and forests'],\n",
    "    'Shorebirds': ['submerged lake']\n",
    "}   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def pivot_year(df, wit_metric, pkey='UID'):\n",
    "    \"\"\"\n",
    "        Pivot the input data to a dataframe with years as column headers\n",
    "        calculate the mean and stddev for the baseline years =1989-2022 excluding the millinium drought\n",
    "        \n",
    "        count =  number of years with waterbird counts\n",
    "        baseline = mean of baseline years\n",
    "        max = maximum value for baseline period\n",
    "        median = median value for baseline period\n",
    "        mad =  median absolute deviation baseline\n",
    "               mad is a non-parametric standard deviation used with the\n",
    "               waterbird data because there are lots of spatial units with no or few count records\n",
    "    '''\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    allyears = df['year'].unique().tolist()\n",
    "    baseline = [y for y in allyears if y not in millennium_drought]   #alltime excluding the millennium_drought\n",
    "    \n",
    "    \n",
    "    pivot = df.pivot(index = pkey, columns='year', values=wit_metric )\n",
    "        #count of how many years have data\n",
    "    pivot['count'] = pivot.count(axis=1, numeric_only=True)\n",
    "\n",
    "    #baseline = average metric for baseline years \n",
    "    pivot['baseline'] = pivot[baseline].mean(axis=1, numeric_only=True)\n",
    "\n",
    "    #stddev = stdev in metric for baseline years\n",
    "    pivot['stddev'] = pivot[baseline].std(axis=1, numeric_only=True)\n",
    "    #pivot['dev'+str(recent[-1])] = ((pivot[recent[-1]]-pivot['baseline'])/pivot['stddev'])+0\n",
    "    \n",
    "    #baseline = average metric for baseline years \n",
    "    pivot['max'] = pivot[baseline].max(axis=1, numeric_only=True)\n",
    "    \n",
    "    \n",
    "    pivot['median'] = pivot[baseline].median(axis=1, numeric_only=True)\n",
    "\n",
    "    tmp=pd.DataFrame()\n",
    "    for y in baseline:\n",
    "        tmp[y] = abs(pivot['median'] - pivot[y])\n",
    "    #tmp.to_csv(\"DEBUG_tmp.csv\")\n",
    "    pivot['mad'] = tmp.median(axis=1, numeric_only=True)\n",
    "\n",
    "    #deviation from baseline in each of the recent (last 5 years) standardised by the stddev\n",
    "    #for y in baseline[-5:]:\n",
    "    #    pivot['cond'+str(y)] = (pivot[y]-pivot['baseline'])/pivot['stddev']  #+0 converts -0 to 0\n",
    "\n",
    "    #sum negative scores in recent (last 5)    \n",
    "    #ya = ['status'+str(y) for y in recent[::-1]]\n",
    "    #pivot['sum5y_neg'] = pivot[ya][pivot[ya] < 0].sum(axis=1, numeric_only=True)\n",
    "    return pivot\n",
    "\n",
    "def fn_slope(d):\n",
    "    \"\"\"\n",
    "        calculate the rate of change as the slope through the supplied points\n",
    "        input is a series of values\n",
    "        output is the rate of change\n",
    "    \"\"\"\n",
    "    yvalues = d.values\n",
    "    if len(yvalues) < 2:\n",
    "        return float('NaN')\n",
    "    else:\n",
    "        xvalues = list(range(0,len(yvalues)))\n",
    "        #standardise xvalues to keeps spread of slope roughly -1 to 1  as number of x values varies\n",
    "        xmean=np.mean(xvalues)\n",
    "        xstdev=np.std(xvalues)\n",
    "        standardised_xvalues = [(x-xmean)/xstdev for x in xvalues]\n",
    "        return np.polyfit(standardised_xvalues, yvalues, 1)[0].round(4) #slope\n",
    "\n",
    "def fn_average_trend (df, period, trend_period = None, nobaseline = False):\n",
    "    \"\"\"\n",
    "        calculate the average rate of change within a windows of x years (the trend_period)\n",
    "    \"\"\"\n",
    "    if trend_period is None:\n",
    "        trend_period = period\n",
    "    elif len(trend_period) > len(period):\n",
    "        raise Exception(\"trend_window_width must be less than or equal to year_window_width\")\n",
    "    #calculate mean and slope of the 5years values difference from baseline standardised by the stddev\n",
    "\n",
    "    if nobaseline:\n",
    "        tmp = df[period]\n",
    "    else:\n",
    "        tmp = df[period].subtract(df['baseline'], axis=0).div(df['stddev'], axis=0)+0\n",
    "    #tmp=tmp.dropna()\n",
    "    tmp=tmp.fillna(0) # sites with no variation (eg always dry have baseline=0, stddev=0 so standardised metric becomes na from divide by zero - recast to zero\n",
    "    #debug print ('period', period)\n",
    "    slope = 'Trend'+ str(period[-1])\n",
    "    ave = 'Ave' + str(period[-1]) #capital A ensures column sorts first\n",
    "    tmp[ave] = tmp[period].mean(axis=1, numeric_only=True)\n",
    "    cols = [ave]\n",
    "    if len(period) > 1:\n",
    "        #print ('trend_period', trend_period)\n",
    "        #print(tmp)\n",
    "        tmp[slope] = tmp[trend_period].apply(fn_slope, axis=1)\n",
    "        cols.append(slope)\n",
    "    #debug tmp.to_csv(str(year)+\"fn_average_trend_tmp.csv\")\n",
    "    return tmp[cols]\n",
    "\n",
    "# def aggregate_fields (ANAE, aggshp, aggfield):\n",
    "#     agg = gpd.read_file(aggshp).to_crs(\"EPSG:3577\")[aggfield + ['geometry']]\n",
    "#     grp_labels = gpd.sjoin(ANAE, agg, how=\"left\", op='intersects')[['UID', 'ANAE_TYPE','Area_Ha'] + aggfield].set_index('UID')\n",
    "#     return grp_labels, aggfield\n",
    "\n",
    "#def aggregate_area_weighted(df, agg, aggfield, ANAE = ANAE, ANAEgrp = [], pkey='UID'):\n",
    "def aggregate_area_weighted(df, agg, aggfield, ANAEgrp = []):\n",
    "    \"\"\"\n",
    "        Takes parameter values for individual ANAE ecosystem polygons and aggregates the values to larger areas\n",
    "        using an area weighting.  e.g. to aggregate a metric across all the ANAE polygons within a Ramsar site\n",
    "        inputs: dataframe of parameter values per ANAE polygon\n",
    "                a specified aggregator (one or more larger subunits that contain multiple ANAE polygons)\n",
    "        \n",
    "        Output is a single metric value for each larger area subunit calcualted the area weighted mean of the ANAE polygons within it\n",
    "    \"\"\"\n",
    "    cols = df.columns.values.tolist()\n",
    "    #debug df.to_csv(\"fn_aggregate_area_weighted_df.csv\")\n",
    "    #debug grp_labels.to_csv(\"fn_aggregate_area_weighted_grp_labels.csv\")\n",
    "\n",
    "    agdf = df.join(agg, how='inner')\n",
    "    agdf = agdf.replace([np.inf, -np.inf], np.nan)  #there are some stray \"inf\" values from dividing by very small small stddev -covert to NaN so sum(numeric_only = True) can ignore them\n",
    "\n",
    "    #debug agdf.to_csv(\"fn_aggregate_area_weighted_agdf.csv\")\n",
    "    if agg.name == 'ANAE':\n",
    "        return agdf[['grp'] + cols +['Area_Ha']].set_index('grp', append=True)\n",
    "    else:\n",
    "        agdf[cols] = agdf[cols].multiply(agdf['Area_Ha'], axis = 0)\n",
    "        #debug agdf.to_csv(\"fn_aggregate_area_weighted_agdftimesArea.csv\")\n",
    "        agg_data = agdf[cols +['Area_Ha']+aggfield+ANAEgrp].groupby(aggfield+ANAEgrp).sum(numeric_only = True)\n",
    "        #debug agg_data.to_csv(\"fn_aggregate_area_weighted_agg_data.csv\")\n",
    "        agg_data[cols] = agg_data[cols].div(agg_data['Area_Ha'], axis = 0)\n",
    "        return agg_data[cols+['Area_Ha']]\n",
    "\n",
    "def bin_stress_scores(x, params):\n",
    "    (thresholds, colname) = params\n",
    "    '''\n",
    "    bin values into 3,2,1 (low, medium high) = reverse of condition binning \n",
    "    applying pre-determined thresholds mapped in tsl_score dict.\n",
    "    '''\n",
    "    _bins = thresholds[x['grp'].iat[0]]+[float(\"inf\")]\n",
    "    #debug print (x['grp'].iat[0],_bins,list(range(len(_bins)-1,0,-1)))\n",
    "    return pd.cut(x[colname], bins=_bins, right=False, labels=range(len(_bins)-1,0,-1)).astype('float') #return as float instead of category so we can multiply by area to scale up\n",
    "\n",
    "def rename_stats_columns(c, metric_name):\n",
    "    '''\n",
    "        a clumsy routine in ever evolving code to remname column headers in the data frame\n",
    "    '''\n",
    "    c = c.replace('Ave',metric_name)\n",
    "    c = c.replace('Trend','T'+metric_name)\n",
    "    return c\n",
    "\n",
    "def deviation_from_baseline(df, metric, year_window_width, trend_window_width, nobaseline = False):\n",
    "    '''\n",
    "        calculate the deviation from the baseline in each year of the data frame\n",
    "        \n",
    "        append also the Trend in the deviations over the trend_window_width with prefix \"T\" on columns headings\n",
    "        \n",
    "        append also the SUM deviation over the year_window_width with prefix \"sum\" on columns headings (e.g. sum of the preceeding 5 years)\n",
    "    '''\n",
    "    allyears = df['year'].unique().tolist()\n",
    "    baseline = [y for y in allyears if y not in millennium_drought]   #alltime excluding the millennium_drought\n",
    "    _years=range(allyears[0]+year_window_width-1,allyears[-1]+1)\n",
    "    metric_name = metric.replace('+','').lower()\n",
    "    \n",
    "    pivot = pivot_year(df, metric, pkey).round(4)\n",
    "    pivot.to_csv(f\"BWS_pivot_{metric}.csv\")\n",
    "    dfs = []\n",
    "    for p, year in enumerate(tqdm(_years, desc = f\"{metric} in {year_window_width}y window, trend over {trend_window_width}y:\")):\n",
    "        period = list(range(year-year_window_width+1,year+1))\n",
    "        trend_period = period[-trend_window_width:]\n",
    "        stats_df = fn_average_trend(pivot, period, trend_period, nobaseline = nobaseline)\n",
    "        #debug stats_df.to_csv(str(year)+\"testmetrics.csv\")\n",
    "        #debug print(year,stats_df)\n",
    "        ave = stats_df.columns.values.tolist()[0]\n",
    "        #if we want to score before aggregating\n",
    "        #stats_df['sc'+metric+str(year)] = pd.cut(stats_df[ave], bins = _bins, labels = range(1,len(_bins))).astype('float') #float not default category so can be aggregated\n",
    "        if len(stats_df.columns) > 1:\n",
    "            trend = stats_df.columns.values.tolist()[1]\n",
    "            stats_df['sum'+metric_name+str(year)] = stats_df.sum(axis=1)\n",
    "            #if we want to score before aggregating\n",
    "            #stats_df['scT'+metric+str(year)] = pd.cut(stats_df[ave], bins = _bins, labels = range(1,len(_bins))).astype('float') #float not default category so can be aggregated           \n",
    "        dfs.append(stats_df)\n",
    "    #aggregate all the metrics into a single data frame\n",
    "    metrics_df = pd.concat(dfs, axis=1).sort_index(axis=1)\n",
    "    metrics_df = metrics_df.rename(columns = {c: rename_stats_columns(c.strip(), metric_name) for c in metrics_df.columns})\n",
    "    return metrics_df\n",
    "\n",
    "def append_metric_scores(df, col_list, _bins, _labels):\n",
    "    '''\n",
    "        scores the input data frame into bins and appends the scores for each year as\n",
    "        additonal columns added to the right edge of the data frame so the data can be easily viewed in Excel\n",
    "    \n",
    "    '''\n",
    "    for col in col_list:\n",
    "        df['sc'+col] = pd.cut( df[col], bins = _bins, labels = _labels, include_lowest = True).astype('float') #defaults to 'category' so recast to float so scores can be aggregated\n",
    "    return df\n",
    "\n",
    "def calc_pivots (df, metrics, year_window_width=5, trend_window_width=None, _bins=[float('-inf'),-1,0,float('inf')], reverse_scores=False, nobaseline=False):\n",
    "    '''\n",
    "        This brings together some of the code above to\n",
    "        summarise the metrics in a moving window of multiple years\n",
    "        (for the BWS vulnerabilities project the veg condition and trend (rate of change) over a\n",
    "        moving 5 years period was calculated for each year of the data frame\n",
    "        \n",
    "        The metrics are then aggregated to the pre-defined larger spatial subunits (Ramsar sites, waterbird breeding sites, valleys)\n",
    "        \n",
    "        Pivot tables are written to the working directoy as cvs files for inspection in Excel.\n",
    "        The pivot tables will also be read in and scored for the final integration of vulnerability metrics\n",
    "        \n",
    "        reverse_scores true/false is used to switch the logic for different metrics \n",
    "        e.g. more green veg (WIT pv) is good, more bare soil (WIT bs) is bad\n",
    "    \n",
    "    ''' \n",
    "    \n",
    "    if trend_window_width is None:\n",
    "        trend_window_width = year_window_width\n",
    "    elif trend_window_width > year_window_width:\n",
    "        raise Exception(\"trend_window_width must be less than or equal to year_window_width\")\n",
    "    _scores = list(range(1,len(_bins)))\n",
    "    if reverse_scores: _scores=_scores[::-1]\n",
    "    wit={}\n",
    "    for metric in metrics:\n",
    "        metrics_df = deviation_from_baseline(df, metric, year_window_width, trend_window_width, nobaseline = nobaseline)\n",
    "        print('Aggregate metrics and scores:')\n",
    "        for ag in aggregators:\n",
    "            fname = f\"{metric}_{ag}_{year_window_width}yr.csv\"\n",
    "            print (f\"     {ag} - {fname}\")\n",
    "            wit[ag] = aggregate_area_weighted(metrics_df, aggregators[ag], aggfield[ag], ANAEgrp = ['grp']).round(4)\n",
    "            col_list = [c for c in wit[ag].columns.values.tolist() if c != 'Area_Ha']\n",
    "            wit[ag] = append_metric_scores(wit[ag], col_list, _bins, _scores)\n",
    "            wit[ag].to_csv(fname)\n",
    "\n",
    "def standardise (df):\n",
    "    '''\n",
    "        Standardises an input data frame to values between 0-1\n",
    "        used to standardise NDVI from older NOAA AVHRR and newer MODIS\n",
    "    '''\n",
    "    dmin = df.min()\n",
    "    drange = df.max() - dmin\n",
    "    return df.subtract(dmin).divide(drange)\n",
    "\n",
    "def infill_years(series):\n",
    "    '''\n",
    "      used to fill out a series of years when some years are not represented due to missing data\n",
    "      i.e. wehen there are no waterbird counts for a year in a location\n",
    "    '''\n",
    "    indexnames = list(series.index.names)\n",
    "    #notyear = [x for x in indexnames if x != 'year']\n",
    "    notyear = ','.join(x for x in indexnames if x != 'year')\n",
    "    df=pd.DataFrame(series)\n",
    "\n",
    "    #fill years without breeding with 0\n",
    "    mux = pd.MultiIndex.from_product([\n",
    "            fullperiod,\n",
    "            df.reset_index()[notyear].unique()\n",
    "            ], names=indexnames)\n",
    "    return df.reindex(mux).fillna(0)\n",
    "\n",
    "\n",
    "def get_nearest_distance(left, right, search_distance, name, label):\n",
    "    \"\"\"get distance between to features (passed as left and right)\"\"\"\n",
    "    search_area = left.buffer(search_distance)\n",
    "\n",
    "    data = []\n",
    "    for i in range(len(search_area)):\n",
    "        geom = search_area.geometry.iloc[i]\n",
    "        query = right.sindex.query(geom)\n",
    "        dist = right.iloc[query].distance(left.geometry.iloc[i])\n",
    "        min_dist = dist.min()\n",
    "        if 0 <= min_dist <= search_distance:\n",
    "            right_idx = dist.idxmin()\n",
    "            row = [right_idx, right.iloc[right_idx][label], min_dist]\n",
    "        else:\n",
    "            #no features within search distance\n",
    "            row = [-1, float('NaN'), -1]\n",
    "        data.append(row)\n",
    "    return pd.DataFrame(data, index = left.index, columns=[name + '_idx', name + '_name', name + '_dist'])\n",
    "\n",
    "def waterbird_metrics(df, aggregator, breeding=''):\n",
    "    '''\n",
    "    summarise various waterbird counts\n",
    "    Annual waterbird totals are estimated as the larger number of:\n",
    "        * the maximum count per spatial unit from aerial survey; or\n",
    "        * the combined aggregate of individual observations\n",
    "        \n",
    "    The logic here is that you cant just sum waterbird records because \n",
    "    individual ALA obvservations are counting the same individuals as seen in the aerial surveys\n",
    "    also...  ALA records can also be multiple observations of the same individuals\n",
    "    \n",
    "    '''\n",
    "\n",
    "    big_surveys = ['Murray Icon', 'SEA', 'CLLMM', 'Eastern Australian Survey' , 'Hydrological Indicator Sites']\n",
    "    indivperANAE = df[~df.cCode.isin(big_surveys)].groupby(['year','ANAE_idx', aggregator, 'grp', 'vName'])['iCount'].max()\n",
    "    indivperaggregate = indivperANAE.groupby(['year', aggregator, 'grp', 'vName']).sum()\n",
    "    each_big_surveys_peraggregate = df[df.cCode.isin(big_surveys)].groupby(['year', 'cCode', aggregator, 'grp', 'vName'])['iCount'].sum()\n",
    "    max_big_surveys_peraggregate = each_big_surveys_peraggregate.groupby(['year', aggregator, 'grp', 'vName']).max()\n",
    "\n",
    "    indiv = pd.concat([indivperaggregate,max_big_surveys_peraggregate]).groupby(['year', aggregator, 'grp', 'vName']).max()\n",
    "    grpindiv = indiv.groupby(['year', aggregator, 'grp']).sum()\n",
    "    \n",
    "    sr = df.groupby(['year', aggregator])['vName'].nunique()\n",
    "    grpsr = df.groupby(['year', aggregator, 'grp'])['vName'].nunique()\n",
    "\n",
    "    indiv.to_csv('indivcount'+aggregator+breeding+'.csv')\n",
    "    grpindiv.to_csv('grpindiv'+aggregator+breeding+'.csv')\n",
    "    return indiv, grpindiv, sr, grpsr\n",
    "\n",
    "\n",
    "def extract_scores (ag, fname, score_field_name):\n",
    "    '''\n",
    "        Retrieves the score columns from specified output pivot tables\n",
    "        and strips off the string prefix from year columns.\n",
    "        This standardises the format of the different score tables so metrics\n",
    "        can be summed and counted across groups and features\n",
    "        with pandas append and groupby functions        \n",
    "    '''\n",
    "    #print(f'     Reading scores from {fname}...') #DEBUG\n",
    "    scores = pd.read_csv(fname, low_memory=False)\n",
    "    colnames = [f\"{score_field_name}{y}\" for y in years] \n",
    "    scores = scores[aggfield[ag]+['grp']+colnames].set_index(aggfield[ag]+['grp'])\n",
    "    return scores.rename(columns = {c: c[-4:] for c in scores.columns})\n",
    "\n",
    "def habitat_to_waterbird_groups (ag, score_df):\n",
    "    wb_grp_scores = []\n",
    "    idx =score_df.index.names\n",
    "    #summed aggregate area per spatial unit needed to to area weighting if a waterbird group is spread across more than one habitat types (Cryptic waders)\n",
    "    area = aggregators[ag][['Area_Ha']+['grp']+aggfield[ag]].groupby(aggfield[ag]+['grp']).sum()\n",
    "    #print (area) #DEBUG\n",
    "    for wb_group in waterbird_dom_habitats:\n",
    "        \n",
    "        #get habitat rows from stress_df that correspond to the waterbird group\n",
    "        grp_df = score_df.iloc[score_df.index.get_level_values('grp').isin(waterbird_dom_habitats[wb_group])].copy()\n",
    "        cols = grp_df.columns.tolist()\n",
    "        grp_df =  grp_df.join(area)\n",
    "        grp_df = grp_df.reset_index()  #removing the index makes it easy to rename the habitats to the waterbird groups\n",
    "        #rewrite all the selected habitat types to the waterbird group name\n",
    "        grp_df['grp'] = wb_group\n",
    "        if len(waterbird_dom_habitats[wb_group]) > 1:\n",
    "            # combine scores for multiple habitats using area-weighting\n",
    "            #  print (f'          {waterbird_dom_habitats[wb_group]} assigned to {wb_group} - area weighted average') #DEBUG\n",
    "            grp_df[cols] = grp_df[cols].multiply(grp_df['Area_Ha'], axis = 0)\n",
    "            agg_data = grp_df[cols +['Area_Ha']+idx].groupby(idx).sum(numeric_only = True)\n",
    "            agg_data[cols] = agg_data[cols].div(agg_data['Area_Ha'], axis = 0)\n",
    "            grp_df = agg_data[cols]  #removes the Area_Ha so it doesnt interfere with rescaling the frames to 0-1 later\n",
    "\n",
    "        else:\n",
    "            #print (f'          {waterbird_dom_habitats[wb_group]} assigned to {wb_group}') #DEBUG\n",
    "            grp_df = grp_df.set_index(idx).drop('Area_Ha', axis=1) #removes the Area_Ha so it doesnt interfere with rescaling the frames to 0-1 later\n",
    "        wb_grp_scores.append(grp_df)\n",
    "        #print (grp_df)  #DEBUG\n",
    "    wb_scores = pd.concat(wb_grp_scores)\n",
    "    return wb_scores\n",
    "\n",
    "def normalise_data (df):\n",
    "    df_min = min(df.min())\n",
    "    df_max = max(df.max())\n",
    "    return df.subtract(df_min).divide(df_max-df_min)\n",
    "\n",
    "def sum_and_normalise_data_weighted (df, max_metric_count):\n",
    "    '''\n",
    "        sums the condition/stress scores for each feature in the index and rescales the data\n",
    "        normalising to range 0-1 allowing for cells with missising data\n",
    "        rescaled = (sum - count) / (count * number of possible metrics) - count)\n",
    "    '''\n",
    "    sum_df = df.groupby(level=df.index.names).sum()\n",
    "    count_df = df.groupby(level=df.index.names).count()\n",
    "    return sum_df.subtract(count_df).divide(count_df.multiply(max_metric_count).subtract(count_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the spatial subunits - ANAE and aggregating layers\n",
    "\n",
    "This block of code is slow to run as it:\n",
    "1. first reads in the ANAE polygons\n",
    "2. spatially join the ANAE to multiple data sets with larger-scale subunits to map the aggregations of individual ANAE polgons required to represent larger areas (e.g. Ramsar sites, DIWA wetlands, Valleys\n",
    "\n",
    "The spatial data is read in once and stored and will not be re-read if the cell is re-run.  If the data needs to be read again either reset the notebook at start again or re-run the cell above that initialises the data stores to NONE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ANAE...\n",
      "Reading Dirty Thirty Waterbird areas...\n",
      "Reading DIWA...\n",
      "Reading Ramsar...\n",
      "Reading BWS valleys...\n"
     ]
    }
   ],
   "source": [
    "#name of the unique ID identifying each ANAEv3 polygon is a 9 character geohash \n",
    "pkey = 'UID'\n",
    "\n",
    "#dictionary of the aggregators - for the BWS vulnerabilites project we were interested in scaling up from ANAE polygons to larger subunits including\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#DTwaterbirds =  dirty thirty =  MDBA waterbird units\n",
    "#DIWA = Directory of Important Wetlands \n",
    "#Ramsar\n",
    "#Valleys are valleys used by MDBA uses as \"vegetation regions\" to guide priority setting in the BWS\n",
    "#Basin is the whole MDB\n",
    "\n",
    "#aggfield is the unique identifier for subunits within each of the aggregator data sets\n",
    "# note for Ramsar sites we can aggregate ANAE polygons to multiple \"wetlands\" within each ramsar site\n",
    "aggfield = {\n",
    "    'ANAE': ['UID'],\n",
    "    'DTwaterbirds': ['LABEL'],\n",
    "    'DIWA': ['WNAME'],\n",
    "    'Ramsar': ['RAMSAR_NAM','WETLAND_NA'],\n",
    "    'Valley': ['BWS_Region'],\n",
    "    'Basin' : []\n",
    "}\n",
    "\n",
    "#spatial data is read into geopandas data frames  \n",
    "\n",
    "#if we havn't read the ANAE data in before the do it now reading into a geopandas frame\n",
    "#for the ANAE we simplify the typology by grouping some of the ANAE ecosystem types with the same dominant vegetation\n",
    "#e.g. we take black box floodplains and blackbox woodland swamps and combine into a single \"black box\" class\n",
    "  \n",
    "if ANAE is None:\n",
    "    print ('Reading ANAE...')\n",
    "    ANAE = gpd.read_file(os.path.join(spatial_path, 'ANAEv3_BWS.shp')).to_crs(\"EPSG:3577\")\n",
    "    ANAE = ANAE[['UID','ANAE_TYPE','Area_Ha','geometry']]\n",
    "    \n",
    "    ANAE.loc[ANAE['ANAE_TYPE'].str.contains('river red gum', case=False), 'grp'] = 'river red gum swamps and forests'\n",
    "    ANAE.loc[(ANAE['ANAE_TYPE'].str.contains('river red gum', case=False)) &\n",
    "             (ANAE['ANAE_TYPE'].str.contains('woodland', case=False)), 'grp'] = 'river red gum woodland'\n",
    "    ANAE.loc[ANAE['ANAE_TYPE'].str.contains('black box', case=False), 'grp'] = 'black box'\n",
    "    ANAE.loc[ANAE['ANAE_TYPE'].str.contains('coolibah', case=False), 'grp'] = 'coolibah'\n",
    "    ANAE.loc[ANAE['ANAE_TYPE'].str.contains('lignum', case=False), 'grp'] = 'lignum'\n",
    "    \n",
    "    ANAE.loc[ANAE['ANAE_TYPE'].str.contains('permanent lake|permanent wetland|aquatic bed', case=False), 'grp'] = 'submerged lake'\n",
    "    ANAE.loc[ANAE['ANAE_TYPE'].str.contains('tall emergent marsh', case=False), 'grp'] = 'tall reed beds'\n",
    "    ANAE.loc[(ANAE['ANAE_TYPE'].str.contains('grass|meadow', case=False)), 'grp'] = 'grassy meadows'\n",
    "    ANAE.loc[ANAE['ANAE_TYPE'].str.contains('forb marsh|temporary wetland|temporary lake', case=False), 'grp'] = 'herbfield'\n",
    "    ANAE.loc[ANAE['ANAE_TYPE'].str.contains('clay', case=False), 'grp'] = 'clay pan'\n",
    "    #setting the names of the data frames is important to permit lookup of appropriate aggregate fields in aggfield dictionary \n",
    "    ANAE.name = 'ANAE' #used to area-weight aggregate all ANAE polygons\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Load Aggregator shapefiles and use spatial joins to determine the ANAE polygons\n",
    "# that are within each larger spatial unit. In theory we could have saved these\n",
    "# as lookup tables for speedy re-use but left coded this way for possible flexibility in the future\n",
    "# if waterbird boundaries or Ramsar boundaries change\n",
    "\n",
    "\n",
    "if DTwaterbirds is None:\n",
    "    print ('Reading Dirty Thirty Waterbird areas...')\n",
    "    DTwaterbirds = gpd.read_file(os.path.join(spatial_path, 'waterbirds_dirty_thirty.shp')).to_crs(\"EPSG:3577\")\n",
    "    DTwaterbirds = gpd.sjoin(ANAE, DTwaterbirds[aggfield['DTwaterbirds'] + ['geometry']], how=\"left\", op='intersects').dropna().set_index('UID')\n",
    "    DTwaterbirds.name = 'DTwaterbirds'\n",
    "if DIWA is None:\n",
    "    print ('Reading DIWA...')\n",
    "    DIWA = gpd.read_file(os.path.join(spatial_path, 'DIWA_complex.shp')).to_crs(\"EPSG:3577\")\n",
    "    DIWA = gpd.sjoin(ANAE, DIWA[aggfield['DIWA'] + ['geometry']], how=\"left\", op='intersects').dropna().set_index('UID')\n",
    "    DIWA.name = 'DIWA'\n",
    "if Ramsar is None:\n",
    "    print ('Reading Ramsar...')\n",
    "    Ramsar = gpd.read_file(os.path.join(spatial_path, 'ramsar_wetlands.shp')).to_crs(\"EPSG:3577\")\n",
    "    Ramsar = gpd.sjoin(ANAE, Ramsar[aggfield['Ramsar'] + ['geometry']], how=\"left\", op='intersects').dropna().set_index('UID')\n",
    "    Ramsar.name = 'Ramsar'\n",
    "if Valley is None:\n",
    "    print ('Reading BWS valleys...')\n",
    "    Valley = gpd.read_file(os.path.join(spatial_path, 'BWSRegions.shp')).to_crs(\"EPSG:3577\")\n",
    "    Valley = gpd.sjoin(ANAE, Valley[aggfield['Valley'] + ['geometry']], how=\"left\", op='intersects').dropna().set_index('UID')\n",
    "    Valley = Valley[Valley['Area_Ha'].notnull()]    \n",
    "    Valley.name = 'Valley'\n",
    "\n",
    "if ANAE.index.name != 'UID':\n",
    "    ANAE = ANAE.set_index('UID')\n",
    "    ANAE.name = 'ANAE'\n",
    "Basin = ANAE[['grp', 'Area_Ha']]\n",
    "Basin.name = 'Basin'\n",
    "\n",
    "\n",
    "aggregators = {'ANAE': ANAE, 'DTwaterbirds':DTwaterbirds, 'DIWA': DIWA, 'Ramsar': Ramsar, 'Valley': Valley, 'Basin': Basin} \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['F4: Unspecified riparian zone or floodplain',\n",
       "       'Etd1.2.1: Tide dominated saltmarsh',\n",
       "       'F2.4: Shrubland riparian zone or floodplain',\n",
       "       'Etd1.3.3: Tide dominated estuary',\n",
       "       'Ewd1.2.3: Intertidal saltmarsh',\n",
       "       'Ewd1.2.4: Intertidal mudflat or sand bar',\n",
       "       'Pt1.8.2: Temporary shrub swamp', 'Ewd1.3.2: Coastal lagoon',\n",
       "       'Etd1.2.2: Tide dominated mudflats and sandbar',\n",
       "       'Pst4: Temporary saline wetland', 'Pst2.2: Temporary salt marsh',\n",
       "       'Etd1.2.3: Tide dominated forest',\n",
       "       'Rp1.4: Permanent lowland stream',\n",
       "       'Rt1.4: Temporary lowland stream',\n",
       "       'Pst1.1: Temporary saline swamp',\n",
       "       'Etd1.1.1: Tide dominated rocky shoreline',\n",
       "       'Ewd1.2.5: Intertidal rocky shoreline',\n",
       "       'Pt1.5.2: Temporary paperbark swamp',\n",
       "       'Rt1.2: Temporary transitional zone stream',\n",
       "       'Pt1.6.2: Temporary woodland swamp',\n",
       "       'F1.13: Paperbark riparian zone or floodplain',\n",
       "       'Lsp1.1: Permanent saline lake', 'Psp1.1: Saline paperbark swamp',\n",
       "       'Psp4: Permanent saline wetland', 'Pst3.2: Salt pan or salt flat',\n",
       "       'Psp2.1: Permanent salt marsh',\n",
       "       'F1.12: Woodland riparian zone or floodplain',\n",
       "       'Pt1: Temporary swamps', 'Rt1: Temporary stream',\n",
       "       'Rp1.2: Permanent transitional zone stream',\n",
       "       'Rp1: Permanent stream',\n",
       "       'F1.11: River cooba woodland riparian zone or floodplain',\n",
       "       'Rt1.1: Temporary high energy upland stream',\n",
       "       'Rt1.3: Temporary low energy upland stream',\n",
       "       'Ru1: Unspecified river',\n",
       "       'Rp1.1: Permanent high energy upland stream',\n",
       "       'Lst1.1: Temporary saline lake', 'Pu1: Unspecified wetland',\n",
       "       'Rp1.3: Permanent low energy upland stream',\n",
       "       'Pp3: Peat bog or fen marsh', 'Pp1.1.2: Permanent paperbark swamp'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is a debug check to list the ANAE types that are NOT assigned to a functional group to see if we missed anything obvious\n",
    "# the types that list here are the types we *don't* include in the determination of wetland/floodplain/waterbird vulnerabilities\n",
    "# (e.g. includes rivers and streams, saline systems)\n",
    "\n",
    "ANAE[ANAE['grp'].isna()]['ANAE_TYPE'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the WIT annual metrics\n",
    "Read in the WIT yearly metrics and calculate pivot tables (note this is slow process but a progress bar is shown).  After the initial calculation of metrics per ANAE polygon is complete the data are aggregated to the various larger spatial scales.\n",
    "\n",
    "\n",
    "**input:** WIT Yearly statistics generated by the wit_metrics notebook as file **RESULT_ANAE_yearly_metrics.csv**\n",
    "\n",
    "**output:** pivot tables for each WIT metric x spatial aggregator combination as CSV files in the working directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading WIT metrics...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aaf0e9bede04df9a1c65a30cfd1873d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "water+wet_median in 5y window, trend over 2y::   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate metrics and scores:\n",
      "     ANAE - water+wet_median_ANAE_5yr.csv\n",
      "     DTwaterbirds - water+wet_median_DTwaterbirds_5yr.csv\n",
      "     DIWA - water+wet_median_DIWA_5yr.csv\n",
      "     Ramsar - water+wet_median_Ramsar_5yr.csv\n",
      "     Valley - water+wet_median_Valley_5yr.csv\n",
      "     Basin - water+wet_median_Basin_5yr.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec1e0a7abd447b3a16e3f2aa58db114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pv_median in 5y window, trend over 2y::   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate metrics and scores:\n",
      "     ANAE - pv_median_ANAE_5yr.csv\n",
      "     DTwaterbirds - pv_median_DTwaterbirds_5yr.csv\n",
      "     DIWA - pv_median_DIWA_5yr.csv\n",
      "     Ramsar - pv_median_Ramsar_5yr.csv\n",
      "     Valley - pv_median_Valley_5yr.csv\n",
      "     Basin - pv_median_Basin_5yr.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc71e118a5364b15aec6d073fcce83dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "npv_median in 5y window, trend over 2y::   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate metrics and scores:\n",
      "     ANAE - npv_median_ANAE_5yr.csv\n",
      "     DTwaterbirds - npv_median_DTwaterbirds_5yr.csv\n",
      "     DIWA - npv_median_DIWA_5yr.csv\n",
      "     Ramsar - npv_median_Ramsar_5yr.csv\n",
      "     Valley - npv_median_Valley_5yr.csv\n",
      "     Basin - npv_median_Basin_5yr.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559345ce40b142dfba6e07f1521139b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "npv+pv+wet_median in 5y window, trend over 2y::   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate metrics and scores:\n",
      "     ANAE - npv+pv+wet_median_ANAE_5yr.csv\n",
      "     DTwaterbirds - npv+pv+wet_median_DTwaterbirds_5yr.csv\n",
      "     DIWA - npv+pv+wet_median_DIWA_5yr.csv\n",
      "     Ramsar - npv+pv+wet_median_Ramsar_5yr.csv\n",
      "     Valley - npv+pv+wet_median_Valley_5yr.csv\n",
      "     Basin - npv+pv+wet_median_Basin_5yr.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20bd7769d7e14145b8e663555978c571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bs_median in 5y window, trend over 2y::   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate metrics and scores:\n",
      "     ANAE - bs_median_ANAE_5yr.csv\n",
      "     DTwaterbirds - bs_median_DTwaterbirds_5yr.csv\n",
      "     DIWA - bs_median_DIWA_5yr.csv\n",
      "     Ramsar - bs_median_Ramsar_5yr.csv\n",
      "     Valley - bs_median_Valley_5yr.csv\n",
      "     Basin - bs_median_Basin_5yr.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b221e4d6da8040d698638c3dc418978b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "water+wet_median in 1y window, trend over 1y::   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate metrics and scores:\n",
      "     ANAE - water+wet_median_ANAE_1yr.csv\n",
      "     DTwaterbirds - water+wet_median_DTwaterbirds_1yr.csv\n",
      "     DIWA - water+wet_median_DIWA_1yr.csv\n",
      "     Ramsar - water+wet_median_Ramsar_1yr.csv\n",
      "     Valley - water+wet_median_Valley_1yr.csv\n",
      "     Basin - water+wet_median_Basin_1yr.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268d3375a9a5492ba6a3f4d5fed161f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pv_median in 1y window, trend over 1y::   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate metrics and scores:\n",
      "     ANAE - pv_median_ANAE_1yr.csv\n",
      "     DTwaterbirds - pv_median_DTwaterbirds_1yr.csv\n",
      "     DIWA - pv_median_DIWA_1yr.csv\n",
      "     Ramsar - pv_median_Ramsar_1yr.csv\n",
      "     Valley - pv_median_Valley_1yr.csv\n",
      "     Basin - pv_median_Basin_1yr.csv\n"
     ]
    }
   ],
   "source": [
    "if wit_yearly is None:   #read the data in if hasn't been read in already\n",
    "    #use geopandas instead of pandas so we can later call on the 'name' attribute whcih is not in the version of pandas were using\n",
    "    print ('Reading WIT metrics...')\n",
    "    wit_yearly = pd.read_csv(os.path.join(data_path,'RESULT_WIT_ANAE_yearly_metrics.csv')).rename(columns={'feature_id':'UID'})\n",
    "    #remove all records not on the managed floodplain\n",
    "    wit_yearly = wit_yearly[wit_yearly['UID'].isin(ANAE.index)]\n",
    "\n",
    "#debug code = limit to first 1000 records so it runs quickly    \n",
    "#wit_yearly = wit_yearly.head(1000)\n",
    "\n",
    "allyears = wit_yearly['year'].unique().tolist()  #list of all years included in the data frame\n",
    "baseline = [y for y in allyears if y not in millennium_drought]   #alltime excluding the millennium_drought\n",
    "\n",
    "\n",
    "\n",
    "# we pass a subset of metrics that we are interested in to the \"calc_pivots\" function\n",
    "#for the BWS project we calculated vegetation stress metrics in a 5year moving window looking at the trend\n",
    "# (rate and direction of change) within the most recent  the last 2 years\n",
    "#the summary pivot tables for each metric x aggregator combination are written to the working directory as CSV files\n",
    "\n",
    "metrics =  ['water+wet_median','pv_median', 'npv_median', 'npv+pv+wet_median']\n",
    "\n",
    "# water+wet_median represents inundation\n",
    "# pv_median =  fractional cover of green vegetation\n",
    "# npv =  fractional cover of non-green (brown) vegetation\n",
    "# npv+pv+wet_median = all vegetation (green, brown and wet vegetation)\n",
    "\n",
    "\n",
    "#npv+pv+wet =  combines all WIT vegetation, water+wet_median represents inundation\n",
    "calc_pivots (wit_yearly, metrics, year_window_width=veg_window_width, trend_window_width=veg_trend_width)\n",
    "\n",
    "#median bare soil (bs_median) is processed separately with the reverse_scores=True switch to reverse\n",
    "# because more bare soil represents declining condition\n",
    "\n",
    "metrics = ['bs_median'] \n",
    "calc_pivots (wit_yearly, metrics, year_window_width=veg_window_width, trend_window_width=veg_trend_width, reverse_scores=True) \n",
    "\n",
    "\n",
    "#waterbirds respond quicker than trees so the BWS project used annual metrics (year_window_width=1) and there is no multi-year trend\n",
    "metrics =  ['water+wet_median', 'pv_median']\n",
    "calc_pivots (wit_yearly, metrics, year_window_width=1) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the WIT Time since last inundation\n",
    "Read in the WIT time since last inundation metrics and score the stress using the user defined thresholds for HIGH, MEDIUM and LOW stress that are defined in the code above (vegetation_tsli_score)\n",
    "\n",
    "**input:** WIT Time since last inundation statistics generated by the wit_metrics notebook as file **'RESULT_ANAE_time_since_last_inundation.csv**\n",
    "\n",
    "**output:** pivot tables for each spatial aggregator with the average time since last inundation in each calendar year for different vegetation groupings scored on a scale of 1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading WIT time since last inundation...\n",
      "Reading WIT inundation metrics...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae1c562621c417ea0536e392d9ed2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Time since last inundation per year:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d4c6bf5a144d989347285205451f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Score time since last inundation for vegetation_stress:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate metrics and scores:\n",
      "     ANAE - time_since_last_inundation_ANAE_vegetation_stress.csv\n",
      "     DTwaterbirds - time_since_last_inundation_DTwaterbirds_vegetation_stress.csv\n",
      "     DIWA - time_since_last_inundation_DIWA_vegetation_stress.csv\n",
      "     Ramsar - time_since_last_inundation_Ramsar_vegetation_stress.csv\n",
      "     Valley - time_since_last_inundation_Valley_vegetation_stress.csv\n",
      "     Basin - time_since_last_inundation_Basin_vegetation_stress.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da80c987353f44bd9dcb0812a9ab6d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Score time since last inundation for waterbird_habitat_stress:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate metrics and scores:\n",
      "     ANAE - time_since_last_inundation_ANAE_waterbird_habitat_stress.csv\n",
      "     DTwaterbirds - time_since_last_inundation_DTwaterbirds_waterbird_habitat_stress.csv\n",
      "     DIWA - time_since_last_inundation_DIWA_waterbird_habitat_stress.csv\n",
      "     Ramsar - time_since_last_inundation_Ramsar_waterbird_habitat_stress.csv\n",
      "     Valley - time_since_last_inundation_Valley_waterbird_habitat_stress.csv\n",
      "     Basin - time_since_last_inundation_Basin_waterbird_habitat_stress.csv\n"
     ]
    }
   ],
   "source": [
    "def score_tsli (df, name, thresholds):\n",
    "    '''\n",
    "        bins the time since last inundation metrics per polygon per year in df\n",
    "        into scores (1,2,3) appending the scores to the data frame\n",
    "        then aggregates the ANAE polygon score to the larger spatial scales\n",
    "    '''\n",
    "    #print(f\"score time since last inundation for {name}\")\n",
    "    for y in tqdm(alltime, desc = f\"Score time since last inundation for {name}\"):\n",
    "        df[f\"sc_tsli{y}\"] = df[['grp',f\"tsli{y}\"]].groupby('grp').apply(bin_stress_scores, (thresholds, f\"tsli{y}\")).reset_index(level= 0, drop= True) #bin_stress_scores is a function declared in this notebook\n",
    "    cols = [f\"tsli{y}\" for y in alltime]+[f\"sc_tsli{y}\" for y in alltime]\n",
    "    print('Aggregate metrics and scores:')\n",
    "    for ag in aggregators:\n",
    "        fname = f\"time_since_last_inundation_{ag}_{name}.csv\"\n",
    "        print (f\"     {ag} - {fname}\")\n",
    "        tsli_ag = aggregate_area_weighted(df[cols], aggregators[ag], aggfield[ag], ANAEgrp = ['grp'])\n",
    "        tsli_ag.round(1).to_csv(fname)\n",
    "\n",
    "\n",
    "\n",
    "if tsli_master is None:   #read the data in if required otherwise re-use\n",
    "    print ('Reading WIT time since last inundation...')\n",
    "    tsli_master = pd.read_csv(os.path.join(data_path,'RESULT_WIT_ANAE_time_since_last_inundation.csv')).rename(columns={'feature_id':'UID'}).set_index('UID')\n",
    "if idf_master is None:\n",
    "    print ('Reading WIT inundation metrics...')\n",
    "    idf_master = pd.read_csv(os.path.join(data_path,'RESULT_WIT_ANAE_inundation_metrics.csv'), parse_dates=['start_time', 'end_time']).rename(columns={'feature_id':'UID'})  #, converters = dict(duration=pd.to_timedelta, gap=pd.to_timedelta)\n",
    "    #much faster to convert these data types once the dataframe is loaded into pandas compared to using converters on csv read\n",
    "    idf_master['duration'] = pd.to_timedelta(idf_master['duration']).dt.days\n",
    "    idf_master['gap'] = pd.to_timedelta(idf_master['gap']).dt.days\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tsli_df=tsli_master.join(ANAE[['ANAE_TYPE', 'grp', 'Area_Ha']])\n",
    "\n",
    "\n",
    "for y in tqdm(alltime, desc = 'Time since last inundation per year'):\n",
    "    cutoff_date = np.datetime64(str(y)+'-12-31')\n",
    "\n",
    "    idf = idf_master[idf_master['start_time']<cutoff_date].copy()\n",
    "    idf.loc[idf['end_time']>cutoff_date, 'end_time'] = cutoff_date\n",
    "    idf.loc[idf['end_time']==cutoff_date, 'duration'] = (idf['end_time'] - idf['start_time'])\n",
    "\n",
    "    timesincelast = idf[(idf['end_time']==idf.groupby('UID')['end_time'].transform('max'))].copy().set_index('UID')\n",
    "    timesincelast=timesincelast.join(tsli_df['final-date'].astype('datetime64[D]'))\n",
    "    timesincelast.loc[timesincelast['final-date'] > cutoff_date, 'final-date'] = cutoff_date\n",
    "    #tsli for this year\n",
    "    tsli_df[f\"tsli{y}\"] = (timesincelast['final-date'] - timesincelast['end_time']).dt.days\n",
    "    \n",
    "#time since last inundation of different vegetation functional groups scored by user defined thresholds\n",
    "    \n",
    "score_tsli(tsli_df, 'vegetation_stress', vegetation_tsli_stress_thresholds)\n",
    "\n",
    "score_tsli(tsli_df, 'waterbird_habitat_stress', waterbird_habitat_stress_thresholds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process NDVI\n",
    "Data obtained from two different Google Earth Engine data sets required to represent the full time period\n",
    "\n",
    "* 1986_1999 - NOAA AVHRR  https://developers.google.com/earth-engine/datasets/catalog/NOAA_CDR_AVHRR_NDVI_V5\n",
    "\n",
    "* 2000_2022 - MODIS https://developers.google.com/earth-engine/datasets/catalog/MODIS_061_MOD13Q1\n",
    "\n",
    "\n",
    "\n",
    "The data sets are not directly comparable with NDVI values obtained from AVHRR being approx 50% of MODIS\n",
    "(a function of the data ranges as provided in Google's earth engine data library).\n",
    "\n",
    "Each data set is therefore standardised to range 0-1 before appending them\n",
    "\n",
    "**input:** average NDVI per ANAE polygon per year as CSV files **NDVI_1986_1999.csv** (NOAA AVHRR) and **NDVI_2000_2022.csv** (MODIS) where each file is the \n",
    "**output:** NDVI pivot tables for each spatial aggregation  as CSV files in the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading AVHRR_NDVI data...\n",
      "Reading MODIS_NDVI data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c8cd4ac17b487db304e1f96ee0bba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NDVI in 5y window, trend over 2y::   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate metrics and scores:\n",
      "     ANAE - NDVI_ANAE_5yr.csv\n",
      "     DTwaterbirds - NDVI_DTwaterbirds_5yr.csv\n",
      "     DIWA - NDVI_DIWA_5yr.csv\n",
      "     Ramsar - NDVI_Ramsar_5yr.csv\n",
      "     Valley - NDVI_Valley_5yr.csv\n",
      "     Basin - NDVI_Basin_5yr.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001d4860b98143c3b3ecac2c64308473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NDVI in 1y window, trend over 1y::   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate metrics and scores:\n",
      "     ANAE - NDVI_ANAE_1yr.csv\n",
      "     DTwaterbirds - NDVI_DTwaterbirds_1yr.csv\n",
      "     DIWA - NDVI_DIWA_1yr.csv\n",
      "     Ramsar - NDVI_Ramsar_1yr.csv\n",
      "     Valley - NDVI_Valley_1yr.csv\n",
      "     Basin - NDVI_Basin_1yr.csv\n"
     ]
    }
   ],
   "source": [
    "if ndvi is None:  #read the data in if required otherwise re-use\n",
    "    print ('Reading AVHRR_NDVI data...')\n",
    "    #NOAA_CDR_AVHRR_NDVI_V5 range -9998 to 9998\n",
    "    ndvi1 = pd.read_csv(os.path.join(data_path,'NDVI_1986_1999.csv'), dtype={'NDVI': float, 'UID': str, 'year': int})\n",
    "    ndvi1['NDVI'] = standardise(ndvi1['NDVI'])\n",
    "    \n",
    "    #MODIS_061_MOD13Q1 range -9998 to 10000\n",
    "    print ('Reading MODIS_NDVI data...')\n",
    "    ndvi2 = pd.read_csv(os.path.join(data_path,'NDVI_2000_2022.csv'), dtype={'NDVI': float, 'UID': str, 'year': int})\n",
    "    ndvi2['NDVI'] = standardise(ndvi2['NDVI'])\n",
    "    \n",
    "    ndvi = pd.concat([ndvi1, ndvi2], ignore_index=True)\n",
    "\n",
    "allyears = ndvi['year'].unique().tolist()  #list of all years included in the data frame\n",
    "baseline = [y for y in allyears if y not in millennium_drought]   #allyears excluding the millennium_drought\n",
    "\n",
    "# we pass a subset of metrics that we are interested in to the \"calc_pivots\" function\n",
    "#for the BWS project we calculated vegetation stress metrics in a 5year moving window looking at the trend\n",
    "# (rate and direction of change) within the most recent  the last 2 years\n",
    "#the summary pivot tables for each metric x aggregator combination are written to the working directory as CSV files\n",
    "\n",
    "metrics = ['NDVI']\n",
    "calc_pivots (ndvi, metrics, year_window_width=veg_window_width, trend_window_width=veg_trend_width)\n",
    "\n",
    "#waterbirds respond quicker than trees so the BWS project used annual metrics (year_window_width=1) and there is no multi-year trend\n",
    "calc_pivots (ndvi, metrics, year_window_width=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Soil Moisture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading root zone soil moisture data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93d84201b114b628260f63252b150c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "soilmoist in 5y window, trend over 2y::   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate metrics and scores:\n",
      "     ANAE - soilmoist_ANAE_5yr.csv\n",
      "     DTwaterbirds - soilmoist_DTwaterbirds_5yr.csv\n",
      "     DIWA - soilmoist_DIWA_5yr.csv\n",
      "     Ramsar - soilmoist_Ramsar_5yr.csv\n",
      "     Valley - soilmoist_Valley_5yr.csv\n",
      "     Basin - soilmoist_Basin_5yr.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c87686bcdd4a23a0f8bd97670213c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "soilmoist in 1y window, trend over 1y::   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate metrics and scores:\n",
      "     ANAE - soilmoist_ANAE_1yr.csv\n",
      "     DTwaterbirds - soilmoist_DTwaterbirds_1yr.csv\n",
      "     DIWA - soilmoist_DIWA_1yr.csv\n",
      "     Ramsar - soilmoist_Ramsar_1yr.csv\n",
      "     Valley - soilmoist_Valley_1yr.csv\n",
      "     Basin - soilmoist_Basin_1yr.csv\n"
     ]
    }
   ],
   "source": [
    "if soilmoisture_df is None:  #read the data in if required otherwise re-use\n",
    "    print ('Reading root zone soil moisture data...')\n",
    "    soilmoisture_df = pd.read_csv(os.path.join(data_path,'ZonalSt30_soilmoistureanomally.csv'))\n",
    "    soilmoisture_df['year'] = pd.to_datetime(soilmoisture_df['StdTime']).dt.year\n",
    "    soilmoisture_df = soilmoisture_df.rename(columns = {'MEAN':'soilmoist'})\n",
    "    \n",
    "#soil moisture is derived from the soil moisture anomaly data set - use different bins to set scoring _bins=[-1, 0.25, 0.5, 1]\n",
    "# since the metric is already expressed as an anomally we pass nobaseline = True to prevent calculation of baseline average\n",
    "                                  \n",
    "# we pass a subset of metrics that we are interested in to the \"calc_pivots\" function\n",
    "#for the BWS project we calculated vegetation stress metrics in a 5year moving window looking at the trend\n",
    "# (rate and direction of change) within the most recent  the last 2 years\n",
    "#the summary pivot tables for each metric x aggregator combination are written to the working directory as CSV files                                  \n",
    "                                  \n",
    "metrics = ['soilmoist']\n",
    "calc_pivots (soilmoisture_df, metrics, year_window_width=veg_window_width, trend_window_width=veg_trend_width, _bins=[-1, 0.25, 0.5, 1], reverse_scores=False, nobaseline=True) \n",
    "\n",
    "#waterbirds response is faster so windows width =1\n",
    "calc_pivots (soilmoisture_df, metrics, year_window_width=1, _bins=[-1, 0.25, 0.5, 1], reverse_scores=False, nobaseline=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Tree Stand Condition\n",
    "\n",
    "**input:** average Tree Stand Condition per ANAE polygon per year.  These were calculated using Google Earth Engine (link to code provided in intro). File anaev3_condcol_reduceRegions.csv is processed once in the code below to join the ANAE vegetation type to as input data file **tree_stand_condition.csv**\n",
    "\n",
    "**output:**  pivot tables for tree condition (redgum blackbox, coolibah) for each spatial aggregation  as CSV files in the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Tree Stand Condition data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d2c9c3159f46daad7aee4b18aa273f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "meanTSC in 5y window, trend over 2y::   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate metrics and scores:\n",
      "     ANAE - meanTSC_ANAE_5yr.csv\n",
      "     DTwaterbirds - meanTSC_DTwaterbirds_5yr.csv\n",
      "     DIWA - meanTSC_DIWA_5yr.csv\n",
      "     Ramsar - meanTSC_Ramsar_5yr.csv\n",
      "     Valley - meanTSC_Valley_5yr.csv\n",
      "     Basin - meanTSC_Basin_5yr.csv\n"
     ]
    }
   ],
   "source": [
    "if TSC_df is None:  #read the data in if required otherwise re-use\n",
    "    print ('Reading Tree Stand Condition data...')\n",
    "    \n",
    "    # the first time this was run we joined the ANAE ecosystem type to the TreeStandCondition\n",
    "    # data in anaev3_condcol_reduceRegions.csv and rewrote the data as tree_stand_condition.csv\n",
    "    # setting join_wetlands = False skips the join and reads the updated data table\n",
    "    \n",
    "    join_wetlands = False\n",
    "    if join_wetlands:\n",
    "        TSC_df = pd.read_csv(os.path.join(data_path,'anaev3_condcol_reduceRegions.csv')).set_index('UID')\n",
    "        TSC_df['year']=(TSC_df['imgIDstart']/10000+1).astype(int)\n",
    "\n",
    "\n",
    "        ANAE_Tree_subset = ANAE[ANAE['ANAE_TYPE'].str.contains('river red gum|black box|coolibah', case=False)]\n",
    "        TSC_df=TSC_df.merge(ANAE_Tree_subset, how='inner', left_on='UID', right_on='UID').drop('geometry', axis=1)\n",
    "        TSC_df.to_csv(os.path.join(TSCpath,'tree_stand_condition.csv'), encoding='utf-8')\n",
    "\n",
    "\n",
    "    else:\n",
    "        TSC_df = pd.read_csv(os.path.join(data_path,'tree_stand_condition.csv'))\n",
    "    \n",
    "# we pass a subset of metrics that we are interested in to the \"calc_pivots\" function\n",
    "#for the BWS project we calculated vegetation stress metrics in a 5year moving window looking at the trend\n",
    "# (rate and direction of change) within the most recent  the last 2 years\n",
    "#the summary pivot tables for each metric x aggregator combination are written to the working directory as CSV files    \n",
    "    \n",
    "metrics = ['meanTSC']\n",
    "calc_pivots (TSC_df, metrics, year_window_width = veg_window_width, trend_window_width=veg_trend_width)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all Vegetation Scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Condition is the sum of 3 metrics, ['meanTSC', 'npv+pv+wet_median', 'ndvi']\n",
      "Stress is the sum of 3 metrics, ['water+wet_median', 'time_since_last_inundation', 'SoilMoist']\n",
      "\n",
      "The scores from saved pivot files in working_directory D:/BWSVulnerability/output\"\n",
      "\n",
      "Summed scores for condition and stress are rescaled to range 0-1 to allow for missing data in spatial features\n",
      "\n",
      "Vulnerability = condition + stress (rescaled 0-1)\n",
      "\n",
      "A matrix with condition | stress | vulnerability scores is output for each aggregator scale...\n",
      "\n",
      "     ANAE - FINAL_BWSVulnerability_vegetation_ANAE.csv\n",
      "     DTwaterbirds - FINAL_BWSVulnerability_vegetation_DTwaterbirds.csv\n",
      "     DIWA - FINAL_BWSVulnerability_vegetation_DIWA.csv\n",
      "     Ramsar - FINAL_BWSVulnerability_vegetation_Ramsar.csv\n",
      "     Valley - FINAL_BWSVulnerability_vegetation_Valley.csv\n",
      "     Basin - FINAL_BWSVulnerability_vegetation_Basin.csv\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "years = range(alltime[0] + veg_window_width - 1, alltime[-1] + 1)\n",
    "\n",
    "cond = ['meanTSC', 'npv+pv+wet_median', 'ndvi']\n",
    "stress = ['water+wet_median', 'time_since_last_inundation', 'SoilMoist']\n",
    "\n",
    "print (f\"\"\"\n",
    "Condition is the sum of {len(cond)} metrics, {cond}\n",
    "Stress is the sum of {len(stress)} metrics, {stress}\n",
    "\n",
    "The scores from saved pivot files in working_directory {working_directory}\"\n",
    "\n",
    "Summed scores for condition and stress are rescaled to range 0-1 to allow for missing data in spatial features\n",
    "\n",
    "Vulnerability = condition + stress (rescaled 0-1)\n",
    "\n",
    "A matrix with condition | stress | vulnerability scores is output for each aggregator scale...\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "#we process all aggregators so vulnerability can be assessed at any of the scales\n",
    "#for veg the BWS regions are the 'valleys'\n",
    "\n",
    "\n",
    "for ag in aggregators:\n",
    "    fname = f\"FINAL_BWSVulnerability_vegetation_{ag}.csv\"\n",
    "    print (f\"     {ag} - {fname}\")\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        *** CONDITION ***\n",
    "    '''\n",
    "    #load in the scores from the condition metric pivot tables that are contributing to the vulnerability score\n",
    "    cond_a = extract_scores(ag, f\"meanTSC_{ag}_5yr.csv\", 'scsummeantsc')\n",
    "    cond_b = extract_scores(ag, f\"npv+pv+wet_median_{ag}_5yr.csv\", 'scsumnpvpvwet_median')\n",
    "    cond_c = extract_scores(ag, f\"ndvi_{ag}_5yr.csv\", 'scsumndvi')\n",
    "    \n",
    "    #stacking the three metrics vertically in a df with the same index allows us to use groupby sum and count by index feature\n",
    "   \n",
    "    combined_cond_df = pd.concat([cond_a, cond_b, cond_c], axis=0)\n",
    "\n",
    "    # the condition score is the sum of multiple individual metric scores (nominally three for vegetation in this case)\n",
    "    # however can be sum of 2-3 metrics if there are missing data resulting in a lower sum.\n",
    "    # The simple normalise_data function would penailse cells with fewer metrics contributing (thus lower score)\n",
    "    # Therefore use a revised method that weights by the count of metrics contributing to each sum is used   \n",
    "    cond_df = sum_and_normalise_data_weighted(combined_cond_df, len(cond)).round(1)\n",
    "    \n",
    "    '''\n",
    "        *** STRESS ***\n",
    "    '''\n",
    "    #load in the scores from the stress metric pivot tables that are contributing to the vulnerability score\n",
    "    stress_a = extract_scores(ag, f\"water+wet_median_{ag}_5yr.csv\", 'scsumwaterwet_median')\n",
    "    stress_b = extract_scores(ag, f\"time_since_last_inundation_{ag}_vegetation_stress.csv\", 'sc_tsli')\n",
    "    stress_c = extract_scores(ag, f\"SoilMoist_{ag}_5yr.csv\", 'scsumsoilmoist')\n",
    "\n",
    "    #stacking the three metrics vertically in a df with the same index allows us to use groupby to sum and count by index feature\n",
    "    combined_stress_df = pd.concat([stress_a,stress_b,stress_c], axis=0)    \n",
    "\n",
    "    # the stress score is the sum of multiple individual metric scores (nominally three for vegetation in this case)\n",
    "    # however can be sum of 1-2 metrics if there are missing data resulting in a lower sum.\n",
    "    # The simple normalise_data function would penailse cells with fewer metrics contributing (thus lower score)\n",
    "    # Therefore use a revised method that weights by the count of metrics contributing to each sum is used   \n",
    "    stress_df = sum_and_normalise_data_weighted(combined_stress_df, len(stress)).round(1)\n",
    "    \n",
    "    '''\n",
    "        *** VULNERABILITY ***\n",
    "    '''\n",
    "    \n",
    "    #vulnerability is condition + stress normalised to range 0-1\n",
    "    tmp_df = pd.concat([cond_df, stress_df], axis=0)\n",
    "    vul_df = normalise_data(tmp_df.groupby(tmp_df.index).sum()).round(1)\n",
    "    \n",
    "    \n",
    "    #for compatibility with Excel for user data review having headers that are numbers (i.e. years) cause \"issues\"\n",
    "    #rename the columns from numerical years to strings with cond, stress, vul prefix so that Excel treats them as headers\n",
    "    \n",
    "    cond_df = cond_df.rename(columns = {c: f\"cond{c}\" for c in cond_df.columns})\n",
    "    stress_df = stress_df.rename(columns = {c: f\"stress{c}\" for c in stress_df.columns})\n",
    "    vul_df = vul_df.rename(columns = {c: f\"vul{c}\" for c in vul_df.columns})\n",
    "    \n",
    "    \n",
    "    #append columns for condition, stress and vulnerability scores into a single table\n",
    "    \n",
    "    score_df = pd.concat([cond_df, stress_df, vul_df], axis=1)\n",
    "    score_df.sort_index(axis=1).to_csv(fname)\n",
    "    \n",
    "    # NOTE output scale used for vegetaion in the report is Valley scale\n",
    "    # The relevant output file is FINAL_BWSVulnerability_vegetation_Valley.csv\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Waterbirds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in the waterbird data from D:/BWSVulnerability/spatial\\waterbirds_spatialunits.shp\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "    Read in the data\n",
    "    \n",
    "    waterbird count data were indexed/assigned to the nearest aggregator polygon within 1km\n",
    "    this was done once and the result saved so we dont have to do again.\n",
    "    If the data is updated add the new waterbird records to waterbirds.shp and switch join_wetlands=TRUE\n",
    "    to rebuild waterbirds_spatialunits.shp\n",
    "    \n",
    "    1km was chosen as a buffer to capture bird observations that are adjacent to polygons\n",
    "    as the location data isnt very accurate and often bird people record locati\n",
    "    by GPS which can captures the location of the observer in the carpark rather than\n",
    "    the actual bird in the wetland\n",
    "'''\n",
    "# spatially join the waterbirds to the DIWA and DTwaterbirds aggregator locations one time\n",
    "# set join_wetlands = True when the waterbirds.shp gets updated to join  any new records\n",
    "\n",
    "join_wetlands = False\n",
    "\n",
    "#if we havnt already read in the wb file do it now\n",
    "if wb is None:\n",
    "    if join_wetlands:\n",
    "        waterbirds = gpd.read_file(os.path.join(spatial_path,'waterbirds.shp')).to_crs(\"EPSG:3577\")\n",
    "        ANAE = gpd.read_file(os.path.join(spatial_path,'ANAEv3_BWS.shp')).to_crs(\"EPSG:3577\")\n",
    "        DIWA = gpd.read_file(os.path.join(spatial_path,'DIWA_complex.shp')).to_crs(\"EPSG:3577\")\n",
    "        DTwaterbirds = gpd.read_file(os.path.join(spatial_path,'waterbirds_dirty_thirty.shp')).to_crs(\"EPSG:3577\")\n",
    "\n",
    "        wb = waterbirds[waterbirds['year']>=1986].copy()\n",
    "        wb = wb.join(get_nearest_distance(wb, ANAE, 1000, 'ANAE', 'UID'))\n",
    "        print (\"ANAE done\")\n",
    "        #wb = wb.join(get_nearest_distance(wb, ANAE1, 1000, 'ANAE', 'UID'))\n",
    "        wb = wb.join(get_nearest_distance(wb, DIWA, 1000, 'DIWA', 'WNAME'))\n",
    "        print (\"DIWA done\")\n",
    "        wb = wb.join(get_nearest_distance(wb, DTwaterbirds, 1000, 'DTwaterbirds', 'LABEL'))\n",
    "        print (\"DTwaterbirds done\")\n",
    "        wb.to_file(os.path.join(cwdpath,'wb.shp'), encoding='utf-8')\n",
    "    else:\n",
    "        #read in the waterbird data to a geopandas frame\n",
    "        fname = os.path.join(spatial_path,'waterbirds_spatialunits.shp')\n",
    "        print (f\"Reading in the waterbird data from {fname}\")\n",
    "        wb = gpd.read_file(fname).to_crs(\"EPSG:3577\")\n",
    "        wb['year']=wb['year'].astype(int) #ensure years are integers\n",
    "\n",
    "    \n",
    "# pop1percent is the current estimate for 1% of the world wide population\n",
    "# Ramsar uses more than 1% being supported at a location as a measure of site significance\n",
    "# was not used in this project but its here now.\n",
    "pop1percent = pd.read_csv(os.path.join(data_path,'waterbirds_1_percent.csv'), dtype = {'pop1percent': 'Int64'})\n",
    "gsrtot = wb.groupby(['grp'])['vName'].nunique()\n",
    "gsrtot.name='maxGrp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55837052.6731643"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#handy check we're reading in all the right data\n",
    "wb['iCount'].sum()\n",
    "#total number of all waterbirds - as of 8/8/2022 this is 55837052.6731643"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " calculate the total indivduals and species richness per aggregating polygon scale\n",
    " Waterbirds obseevations per ANAE polygon are aggregated to DIWA and the 'Dirty Thirty' MDBA Waterbirds Areas\n",
    " \n",
    " naming convention\n",
    " \n",
    " prefixes\n",
    " \n",
    " i = number of individuals per species  for each spatial unit\n",
    " gi = number of individuals in each functional group (summing all species in the group) for each spatial unit\n",
    " sr = total species richness for each spatial unit\n",
    " gsr = group species richness  for each spatial unit\n",
    " \n",
    " \n",
    " suffix\n",
    " \n",
    " breed =  breeding records only\n",
    "'''\n",
    "\n",
    "\n",
    "#create a bunch of data frames holding the sumamry metrics\n",
    "# waterbird_metrics converts observation data to popn estimates\n",
    "\n",
    "iDIWA, giDIWA, srDIWA, gsrDIWA = waterbird_metrics(wb, 'DIWA_name')\n",
    "\n",
    "iDT, giDT, srDT, gsrDT = waterbird_metrics(wb, 'DT_name')\n",
    "\n",
    "breeding = wb[wb['Breeding'] > 0]\n",
    "\n",
    "iDIWAbreed, giDIWAbreed, srDIWAbreed, gsrDIWAbreed = waterbird_metrics(breeding, 'DIWA_name', 'breeding')\n",
    "iDTbreed, giDTbreed, srDTbreed, gsrDTbreed = waterbird_metrics(breeding, 'DT_name', 'breeding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def writepivot (series, name):\n",
    "    \n",
    "    '''\n",
    "        across the waterbird year data calculate data summaries as:\n",
    "        count =  number of years with waterbird counts\n",
    "        max = maximum annual abundance/richness for baseline period\n",
    "        median = median value for baseline period\n",
    "        mad =  median absolute deviation\n",
    "               mad is a robust non-parametric measure of central tendency used as the 'baseline' estimate with the\n",
    "               waterbird data because there are lots of spatial units with no or few count records\n",
    "        cmadYYYY = the annual MAD condition (deviation from the baseline)\n",
    "        \n",
    "        sc_cmadYYYY =  binned score [1,2,3] 1 = more than 1 deviation below baseline, 2= between -1 and 0, 3 = deviation above baseline\n",
    "        \n",
    "        Significance Criteria (optional)\n",
    "        --------------------------------\n",
    "        The following metrics were explorered as ways to assess significance but\n",
    "        in the end were not used. They are retained in the output tables for future reference\n",
    "        The report instead applied HEVAE critiera to assess importance.\n",
    "        \n",
    "        gt1pop_all = number of years total count exceeds 1% of the world population\n",
    "        gt1pop_recent = number of years total count exceeds 1% of the world population in recent period - (defined as last 5 years of the baseline period currently)\n",
    "        grpHistImp = number of years group represented at a location compared to the maximum representation in the whole data set\n",
    "        grpReliability = reliability of presence of group at location as proportion of years that the group is represented\n",
    "        YYYYgrpImp = annual reliability as proportion of maximum for the group\n",
    "        \n",
    "    '''\n",
    "    #waterbirds shapefile uses differnet naming for the DIWA and DT wetlands compared to the aggfield\n",
    "    #need to rename the waterbird grouping columns to match the aggfields names so the waterbird data can easily be matched\n",
    "    #to the vegetation habitat stress metrics WIT, TSC and NDVI etc\n",
    "    series.index.rename({'DT_name':aggfield['DTwaterbirds'][0], 'DIWA_name':aggfield['DIWA'][0]}, inplace=True)\n",
    "    \n",
    "    #summed aggregate area per spatial unit needed to to area weighting if a waterbird group is spread across more than one habitat types (Cryptic waders)\n",
    "    area = aggregators[ag][['Area_Ha']+['grp']+aggfield[ag]].groupby(aggfield[ag]+['grp']).sum()\n",
    "    \n",
    "    pkey = []\n",
    "    pkey = list(x for x in list(series.index.names) if x != 'year')\n",
    "\n",
    "    pivot = pivot_year(pd.DataFrame(series).reset_index(), series.name, pkey)\n",
    "    \n",
    "    for y in alltime:\n",
    "        #calculate condition measures in each year as deviation from baseline (median) stndardised by the MAD\n",
    "        pivot[f\"cmad{y}\"] = pivot[y].subtract(pivot['median']).divide(pivot['mad'].replace({0:1}))\n",
    "        #note: species richess does not alwys vary much so replacing MAD=zero with 1 just for these few records prevents div/zero errors\n",
    "        #and allows retention of missing numerator condition data\n",
    "\n",
    "\n",
    "    #score the condition measures from 1-3  where 1 = more than 1 deviation below, 2= between -1 and 0, 3 = any positive deviation\n",
    "    _bins=[float('-inf'),-1,0,float('inf')]\n",
    "    _scores = list(range(1,len(_bins)))\n",
    "    cols = [f\"cmad{y}\" for y in alltime]\n",
    "    #scores are appended as additonal columns to the right with an 'sc' prefix on the column names\n",
    "    pivot = append_metric_scores(pivot, cols, _bins, _scores)\n",
    "\n",
    "  \n",
    "    # activate or deactivate the additon of these significane criteria\n",
    "    # these were explored but ultimately not used in the project which adopted HEVAE criteria instead\n",
    "    # code left here for prosperity\n",
    "    addSignificantCriteria = False\n",
    "    if addSignificantCriteria:\n",
    "        if 'vName' in pivot.index.names:\n",
    "            pivot = pivot.join(pop1percent.set_index(['vName'])['pop1percent'], on='vName')\n",
    "            for y in alltime:\n",
    "                pivot[f\"{y}prop1pop\"] = pivot[y]/pivot['pop1percent']\n",
    "            #build true-false array - sum=count of true\n",
    "            tmp = pd.DataFrame()\n",
    "            for y in alltime:\n",
    "                tmp[y]=pivot[y]>pivot['pop1percent']\n",
    "            pivot['gt1pop_all'] = tmp.sum(axis=1, numeric_only=True)\n",
    "            pivot['gt1pop_recent'] = tmp[recent].sum(axis=1, numeric_only=True)\n",
    "\n",
    "\n",
    "        elif 'grp' in pivot.index.names and 'sr' in name:\n",
    "\n",
    "            #for species richess condition uses median absolute deviation (mad)\n",
    "\n",
    "            pivot = pivot.join(gsrtot, on='grp')\n",
    "            pivot['grpHistImp'] = pivot['max']/pivot['maxGrp']\n",
    "            pivot['grpReliability'] = pivot['count']/len(alltime)\n",
    "            for y in alltime:\n",
    "                pivot[f\"{y}grpImp\"] = pivot[y]/pivot['maxGrp']\n",
    "            tmp = pd.DataFrame()\n",
    "   \n",
    "    pivot.to_csv(f\"waterbird_{name}_cmad.csv\")\n",
    "    return pivot\n",
    "\n",
    "# write out all the pivot tables containing condition metrics and score at various scales for different data subsets\n",
    "# saved as csv so they can easily be inspected in Excel\n",
    "\n",
    "writepivot (iDIWA, 'iDIWA')\n",
    "writepivot (giDIWA, 'giDIWA')\n",
    "writepivot (srDIWA, 'srDIWA')\n",
    "writepivot (gsrDIWA, 'gsrDIWA')\n",
    "writepivot (iDT, 'iDTwaterbirds')\n",
    "writepivot (giDT, 'giDTwaterbirds')\n",
    "writepivot (srDT, 'srDTwaterbirds')\n",
    "writepivot (gsrDT, 'gsrDTwaterbirds')\n",
    "writepivot (iDIWAbreed, 'iDIWA_breed')\n",
    "writepivot (giDIWAbreed, 'giDIWA_breed')\n",
    "writepivot (srDIWAbreed, 'srDIWA_breed')\n",
    "writepivot (gsrDIWAbreed, 'gsrDIWA_breed')\n",
    "writepivot (iDTbreed, 'iDTwaterbirds_breed')\n",
    "writepivot (giDTbreed, 'giDTwaterbirds_breed')\n",
    "writepivot (srDTbreed, 'srDTwaterbirds_breed')\n",
    "writepivot (gsrDTbreed, 'gsrDTwaterbirds_breed')\n",
    "\n",
    "\n",
    "writepivot (iDT.groupby(['year', 'grp', 'vName']).sum(), 'iBasin')\n",
    "writepivot (iDT.groupby(['year', 'grp']).sum(), 'giBasin')\n",
    "writepivot (iDTbreed.groupby(['year', 'grp', 'vName']).sum(), 'iBasin_breed')\n",
    "writepivot (iDTbreed.groupby(['year', 'grp']).sum(), 'giBasin_breed')\n",
    "\n",
    "#writepivot (wb.groupby(['year'])['vName'].nunique(), 'srBasin')\n",
    "writepivot (wb.groupby(['year', 'grp'])['vName'].nunique(), 'gsrBasin')\n",
    "#writepivot (breeding.groupby(['year', 'grp', 'vName'])['vName'].nunique(), 'srBasinbreed')\n",
    "writepivot (breeding.groupby(['year','grp'])['vName'].nunique(), 'gsrBasin_breed')\n",
    "print ('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Waterbird condition is the sum of 4 metrics, ['gsrBasin', 'giBasin', 'gsrBasinbreed', 'giBasinbreed']\n",
      "    Stress is the sum of 4 metrics, ['water+wet_median', 'time_since_last_inundation', 'SoilMoist', 'pv']\n",
      "    \n",
      "    The scores from saved pivot files in working_directory D:/BWSVulnerability/output\"\n",
      "\n",
      "    Summed scores for condition and stress are rescaled to range 0-1 to allow for missing data in spatial features\n",
      "\n",
      "    Vulnerability = condition + stress / 2\n",
      "\n",
      "    A matrix with condition | stress | vulnerability scores is output for each aggregator scale...\n",
      "    \n",
      "     Basin - FINAL_BWSVulnerability_waterbirds_Basin.csv\n",
      "     DIWA - FINAL_BWSVulnerability_waterbirds_DIWA.csv\n",
      "     DTwaterbirds - FINAL_BWSVulnerability_waterbirds_DTwaterbirds.csv\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "years = alltime\n",
    "#condition of waterbirds is per group at Basin scale based on\n",
    "# species richness, abundance, species breeding, breeding abundance\n",
    "cond = ['gsrBasin', 'giBasin', 'gsrBasinbreed', 'giBasinbreed']\n",
    "\n",
    "stress = ['water+wet_median', 'time_since_last_inundation', 'SoilMoist', 'pv']\n",
    "\n",
    "print (f\"\"\"\n",
    "    Waterbird condition is the sum of {len(cond)} metrics, {cond}\n",
    "    Stress is the sum of {len(stress)} metrics, {stress}\n",
    "    \n",
    "    The scores from saved pivot files in working_directory {working_directory}\"\n",
    "\n",
    "    Summed scores for condition and stress are rescaled to range 0-1 to allow for missing data in spatial features\n",
    "\n",
    "    Vulnerability = condition + stress / 2\n",
    "\n",
    "    A matrix with condition | stress | vulnerability scores is output for each aggregator scale...\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "# we process all aggregators so vulnerability can be assessed at any of the scales\n",
    "# for veg the BWS regions are the 'valleys' but for waterbirds its 'Basin'\n",
    "# we process the DTwaterbirds and DIWA also to inform more site specific inquiry\n",
    "\n",
    "wbaggregators = ['Basin', 'DIWA', 'DTwaterbirds']\n",
    "\n",
    "\n",
    "for ag in wbaggregators:\n",
    "    fname = f\"FINAL_BWSVulnerability_waterbirds_{ag}.csv\"\n",
    "    print (f\"     {ag} - {fname}\")\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        *** CONDITION ***\n",
    "    '''\n",
    "    #load in the scores from the condition metric pivot tables that are contributing to the vulnerability score\n",
    "    cond_a = extract_scores(ag, f'waterbird_gsr{ag}_cmad.csv', 'sccmad')\n",
    "    cond_b = extract_scores(ag, f'waterbird_gi{ag}_cmad.csv', 'sccmad')\n",
    "    cond_c = extract_scores(ag, f'waterbird_gsr{ag}_breed_cmad.csv', 'sccmad')\n",
    "    cond_d = extract_scores(ag, f'waterbird_gi{ag}_breed_cmad.csv', 'sccmad')\n",
    "    \n",
    "    \n",
    "    #write the final condition scores aligned to waterbird groups to files so we can colour them up in Excel for the report\n",
    "    cond_a.to_csv(f'waterbird_{ag}_CONDITION_1_gsr.csv')  \n",
    "    cond_b.to_csv(f'waterbird_{ag}_CONDITION_2_gi.csv')\n",
    "    cond_c.to_csv(f'waterbird_{ag}_CONDITION_3_gsr_breed.csv')\n",
    "    cond_d.to_csv(f'waterbird_{ag}_CONDITION_4_gi_breed.csv')\n",
    "    \n",
    "    \n",
    "    #stacking the 4 metrics vertically in a df with the same index allows us to use groupby sum and count by index feature\n",
    "    combined_cond_df = pd.concat([cond_a, cond_b, cond_c, cond_d], axis=0)\n",
    "    \n",
    "    # the condition score is the sum of multiple individual metric scores (nominally four for waterbirds in this case)\n",
    "    # however can be sum of 2-3 metrics if there are missing data resulting in a lower sum.\n",
    "    # The simple normalise_data function would penailse cells with fewer metrics contributing (thus lower score)\n",
    "    # Therefore use a revised method that weights by the count of metrics contributing to each sum is used\n",
    "    cond_df = sum_and_normalise_data_weighted(combined_cond_df, len(cond)).round(1)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        *** STRESS ***\n",
    "    '''\n",
    "    #load in the scores from the stress metric pivot tables that are contributing to the vulnerability score\n",
    "    stress_a = habitat_to_waterbird_groups (ag, extract_scores(ag, f'water+wet_median_{ag}_1yr.csv', 'scwaterwet_median'))\n",
    "    stress_b = habitat_to_waterbird_groups (ag, extract_scores(ag, f'time_since_last_inundation_{ag}_waterbird_habitat_stress.csv', 'sc_tsli'))\n",
    "    stress_c = habitat_to_waterbird_groups (ag, extract_scores(ag, f'soilmoist_{ag}_1yr.csv', 'scsoilmoist'))\n",
    "    stress_d = habitat_to_waterbird_groups (ag, extract_scores(ag, f'pv_median_{ag}_1yr.csv', 'scpv_median'))\n",
    "\n",
    "    #write the final stress scores aligned to waterbird groups to files so we can colour them up in Excel for the report\n",
    "    stress_a.to_csv(f'waterbird_{ag}_STRESS_1_water+wet_median.csv')\n",
    "    stress_b.to_csv(f'waterbird_{ag}_STRESS_2_sc_tsli.csv')\n",
    "    stress_c.to_csv(f'waterbird_{ag}_STRESS_3_soilmoist.csv')\n",
    "    stress_d.to_csv(f'waterbird_{ag}_STRESS_4_pv_median.csv')\n",
    "\n",
    "    #stacking the 4 metrics vertically in a df with the same index allows us to use groupby to sum and count by index feature\n",
    "    combined_stress_df = pd.concat([stress_a, stress_b, stress_c, stress_d], axis=0)    \n",
    "\n",
    "    # the stress score is the sum of multiple individual metric scores (nominally four for waterbirds in this case)\n",
    "    # however can be sum of 2-3 metrics if there are missing data resulting in a lower sum.\n",
    "    # The simple normalise_data function would penailse cells with fewer metrics contributing (thus lower score)\n",
    "    # Therefore use a revised method that weights by the count of metrics contributing to each sum is used   \n",
    "    stress_df = sum_and_normalise_data_weighted(combined_stress_df, len(stress)).round(1)\n",
    "\n",
    "    \n",
    "    '''\n",
    "        *** VULNERABILITY ***\n",
    "    '''\n",
    "    \n",
    "    #vulnerability is condition + stress normalised to range 0-1\n",
    "    tmp_df = pd.concat([cond_df, stress_df], axis=0)\n",
    "    vul_df = normalise_data(tmp_df.groupby(tmp_df.index).sum()).round(1)\n",
    "    \n",
    "    \n",
    "    #for compatibility with Excel for user data review having headers that are numbers (i.e. years) cause \"issues\"\n",
    "    #rename the columns from numerical years to strings with cond, stress, vul prefix so that Excel treats them as headers\n",
    "    \n",
    "    cond_df = cond_df.rename(columns = {c: f\"cond{c}\" for c in cond_df.columns})\n",
    "    stress_df = stress_df.rename(columns = {c: f\"stress{c}\" for c in stress_df.columns})\n",
    "    vul_df = vul_df.rename(columns = {c: f\"vul{c}\" for c in vul_df.columns})\n",
    "    \n",
    "    \n",
    "    #right-append columns for overall condition, overall stress and vulnerability into a single table\n",
    "    score_df = pd.concat([cond_df, stress_df, vul_df], axis=1)\n",
    "    score_df.sort_index(axis=1).to_csv(fname)\n",
    "    \n",
    "    # NOTE output scale used for waterbirds in the report is Basin scale\n",
    "    # The relevant output file is FINAL_BWSVulnerability_waterbirds_Basin.csv\n",
    "    \n",
    "print(\"All done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
