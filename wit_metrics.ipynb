{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate metrics with WIT data\n",
    "* **Lineage**:  This notebook was derived from code by Geosciences Australia and modified for:\n",
    "* batch input of multiple WIT CVS in a folder (currently the ANAEv3 WIT output includes 270,653 polygons, each with its own csv file)\n",
    "* multiple processor pool support to speed execution when running on a PC workstation\n",
    "* linear interpolation of the observations dates to daily data to improve estimates of inundation duration\n",
    "* some bug fixes in the inundation event metrics that were required when using the interpolated data\n",
    "* output formatting\n",
    "\n",
    "* Original Source: https://github.com/GeoscienceAustralia/dea-notebooks/blob/develop/Scientific_workflows/Wetlands_Insight_Tool/metrics/wit_metrics.ipynb\n",
    "\n",
    "* **Dependencies**: This code requires two things to run (see the analysis parameters section for more information):\n",
    "     * A folder containing pre-calculated WIT csv (obtained for the BWS Priorities Project from Geosciences Australia for each ANAE polygon > 1Ha)\n",
    "     * A shapefile (or equivalent) that contains the area that the WIT result was run over.\n",
    "  \n",
    "     \n",
    "## Background\n",
    "The WIT data are generated by DEA with given wetland polygons and stored in a database on NCI. The data can be dumped into a csv when required. Any statistics can be generated with WIT data. This notebook provides a way in computing temporal statistics (metrics).\n",
    "\n",
    "## WIT Data definition\n",
    "* WIT data provides the following metrics for each polygon unit\n",
    "\n",
    "```\n",
    "   date: time of obersavation\n",
    "   bs: percentage of bare soil\n",
    "   npv: percentage of non photosynthetic vegetation\n",
    "   pv: percentage of green/photosynthetic vegetation\n",
    "   wet: percentage of wetness\n",
    "   water: percentage of water\n",
    "```\n",
    "\n",
    "## Description\n",
    "This notebook uses existing WIT data to compute metrics.\n",
    "* First we load the existing WIT csv data from a saved csv location\n",
    "* Then we compute the metrics for all polygons and output the results to CSV files.  The input CSV files are processed in \"batches\" that are spread across multiple CPU cores.  When execution is complete the various batch outputs are merged together into single result files that contain metrics for every CSV feature ID (ANAE polygons)\n",
    "\n",
    "The following files are created:\n",
    "\n",
    "\n",
    " - **RESULT_WIT_ANAE_yearly_metrics**: min, max, mean, median of each WIT metric per calendar year\n",
    " - **RESULT_WIT_ANAE_event_threshold**: for the BWS project we defined an \"event\" as exceeding the median [water+wet] - this file is read in to calculate the event metrics but could be replaced with user selected values if custom thresholds were wanted or the routine that generates it could be altered to change the threshold formulaiclly.\n",
    " - **RESULT_WIT_ANAE_time_since_last_inundation**: number of days since the inundation event threshold was exceeded\n",
    " - **RESULT_WIT_ANAE_event_times**: start and end time, duration, duration of preceeding gap (the dry period) \n",
    " - **RESULT_WIT_ANAE_event_stats**: for each event calculates the area of the polygon that was wet using the combination water+wet\n",
    " - **RESULT_WIT_ANAE_inundation_metrics**: this is a join of the RESULT_WIT_ANAE_event_time and RESULT_WIT_ANAE_event_stats\n",
    "\n",
    "\n",
    "* WIT metrics: refer [WIT metrics](https://docs.google.com/document/d/1JBZzVRW6K0fJT4jws3lRranPLPBYBkTDvpu94knv5dY/edit?usp=sharing)\n",
    "\n",
    "## Processing Environment\n",
    "For the project the analysis was conducted in the python processing environment of ArcGIS Pro 3.0 but were coded to use common open source python data processing libraries (Geopandas, Pandas, numpy) that should enable the analysis to be repeated in most environments.\n",
    "\n",
    "***\n",
    "     \n",
    "# Contact\n",
    "- Dr Shane Brooks\n",
    "- shane@brooks.eco\n",
    "- https://brooks.eco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Defined Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  set the working directory\n",
    "working_directory = './BWSVulnerability/WIT'\n",
    "\n",
    "\n",
    "# shapefile: the shape file mentioned above to find the  and get their area\n",
    "# set to '' to disable area lookup\n",
    "shapefile = '././BWSVulnerability/WIT/ANAEv3_WIT_clean19042022/ANAEv3_WIT.shp'\n",
    "# shapefile field name that identifies each polygon -  the ANAEv3 UID geohash was used here.\n",
    "# The ANAE UID code is also used in the naming convention for the CSV files\n",
    "skey='UID'\n",
    "\n",
    "# Path with wildcard that defines the list of csv files to process.  When debugging a single filename might be prudent.\n",
    "csvfiles = './BWSVulnerability/WIT/csv'\n",
    "\n",
    "# csv feature_id - the WIT csv output files nclude a column 'feature_id' that in this case is the ANAE UID\n",
    "pkey='feature_id'\n",
    "\n",
    "# whether to interpolate the WIT observation dates (typically 10-50+ per year) to daily data or not.\n",
    "# This is computationally expensive but improves estimates of inundation durataion.\n",
    "interpolate = True\n",
    " #number of WIT csv files to include in each 'batch' for processing by a single CPU core.\n",
    "batchsize = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to the user specified working directory so the worker gets written to the correct location\n",
    "import os\n",
    "os.chdir(working_directory)\n",
    "cwdpath = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write python module workers.py to working directory\n",
    "* To speed up the processing of many CSV files (we intially processed 270,653 ANAE csv) we divide the work across multiple processor cores.  To achieve this the notebook calls an external worker module.  For convienience the worker module code is maintained within the notebook.\n",
    "* Executing the cell below writes out **workers.py** to the current working directoy so that it can be imported as the worker functions for multiprocessing.\n",
    "* the workers module contains the routines for summarising the WIT CSV data\n",
    "* the notebook also contains code that can be run as a single core process in the \"normal way\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting workers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile workers.py\n",
    "\n",
    "\"\"\"\n",
    "    workers.py\n",
    "    \n",
    "    This is a python module that is saved as workers.py to the current working directory when the cell is run in the Jupyter environment\n",
    "    \n",
    "    workers.py is imported in the executable code below this cell for multiprocessing\n",
    "    \n",
    "    This convienently allows the python code of the workers.py module to be edited in the jupyter environment and stored with the multiprocessing code\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fiona\n",
    "from shapely import geometry\n",
    "\n",
    "\n",
    "def shape_list(key, values, shapefile):\n",
    "    \"\"\"\n",
    "        Get a generator of shapes from the given shapefile\n",
    "            key: the key to match in 'properties' in the shape file\n",
    "            values: a list of property values\n",
    "            shapefile: the name of your shape file\n",
    "            e.g. key='ORIGID', values=[1, 2, 3, 4, 5], \n",
    "            shapefile='/g/data/r78/DEA_Wetlands/shapefiles/MDB_ANAE_Aug2017_modified_2019_SB_3577.shp'\n",
    "    \"\"\"\n",
    "    count = len(values)\n",
    "    with fiona.open(shapefile) as allshapes:\n",
    "        for shape in allshapes:\n",
    "            shape_id = shape['properties'].get(key)\n",
    "            if shape_id is None:\n",
    "                continue\n",
    "            if shape_id in values:\n",
    "                yield(shape_id, shape)\n",
    "                count -= 1\n",
    "            if count <= 0:\n",
    "                break\n",
    "    \n",
    "def get_areas(features, pkey='feature_id'):\n",
    "    \"\"\"\n",
    "        Calculate the area of a list/generator of shapes\n",
    "        input:\n",
    "            features: a list of shapes indexed by the key\n",
    "        output:\n",
    "            a dataframe of area index by the key\n",
    "    \"\"\"\n",
    "    re = pd.DataFrame()\n",
    "    for f in features:\n",
    "        va = pd.DataFrame([[f[0], geometry.shape(f[1]['geometry']).area/1e4]], columns=[pkey, 'area'])\n",
    "        re = re.append(va, sort=False)\n",
    "    return re.set_index(pkey)\n",
    "\n",
    "    \n",
    "def annual_metrics(wit_data, members=['pv', 'wet', 'water', 'bs', 'npv', ['npv', 'pv', 'wet'], ['pv', 'wet'], ['water', 'wet']], threshold=[25, 75], pkey='feature_id'):\n",
    "                                              \n",
    "    \"\"\"\n",
    "        Compute the annual max, min, mean, count with given wit data, members and threshold\n",
    "        input:\n",
    "            wit_data: dataframe of WIT\n",
    "            members: the elements which the metrics are computed against, can be a column from wit_data, e.g. 'pv'\n",
    "                         or the sum of wit columns, e.g. ['water', 'wet']\n",
    "            threshold: a list of thresholds such that (elements >= threshold[i]) is True, \n",
    "                        where i = 0, 1...len(threshold)-1\n",
    "        output:\n",
    "            dataframe of metrics\n",
    "    \"\"\"\n",
    "    years = wit_data['date']\n",
    "    i = 0\n",
    "    wit_df = wit_data.copy(deep=True)\n",
    "\n",
    "    for m in members:\n",
    "        if isinstance(m, list):\n",
    "            wit_df.insert(wit_df.columns.size+i, '+'.join(m), wit_df[m].sum(axis=1))\n",
    "    years = pd.DatetimeIndex(wit_df['date']).year.unique()\n",
    "    shape_id_list = wit_df[pkey].unique()\n",
    "    #shane changed 4 to 5 to accomodate median added below \n",
    "    wit_metrics = [pd.DataFrame()] * 5\n",
    "    for y in years:\n",
    "        wit_yearly = wit_df[pd.DatetimeIndex(wit_df['date']).year==y].drop(columns=['date']).groupby(pkey).max()\n",
    "        wit_yearly.insert(0, 'year', y)\n",
    "        wit_yearly = wit_yearly.rename(columns={n: n+'_max' for n in wit_yearly.columns[1:]})\n",
    "        wit_metrics[0] = wit_metrics[0].append(wit_yearly, sort=False)\n",
    "    for y in years:\n",
    "        wit_yearly = wit_df[pd.DatetimeIndex(wit_df['date']).year==y].drop(columns=['date']).groupby(pkey).min()\n",
    "        wit_yearly.insert(0, 'year', y)\n",
    "        wit_yearly = wit_yearly.rename(columns={n: n+'_min' for n in wit_yearly.columns[1:]})\n",
    "        wit_metrics[1] = wit_metrics[1].append(wit_yearly, sort=False)\n",
    "    for y in years:\n",
    "        wit_yearly = wit_df[pd.DatetimeIndex(wit_df['date']).year==y].drop(columns=['date']).groupby(pkey).mean()\n",
    "        wit_yearly.insert(0, 'year', y)\n",
    "        wit_yearly = wit_yearly.rename(columns={n: n+'_mean' for n in wit_yearly.columns[1:]})\n",
    "        wit_metrics[2] = wit_metrics[2].append(wit_yearly, sort=False)\n",
    "        \n",
    "    #*********************** START ADDED BY SHANE ***********************\n",
    "    #adding median\n",
    "    for y in years:\n",
    "        wit_yearly = wit_df[pd.DatetimeIndex(wit_df['date']).year==y].drop(columns=['date']).groupby(pkey).median()\n",
    "        wit_yearly.insert(0, 'year', y)\n",
    "        wit_yearly = wit_yearly.rename(columns={n: n+'_median' for n in wit_yearly.columns[1:]})\n",
    "        wit_metrics[3] = wit_metrics[3].append(wit_yearly, sort=False)\n",
    "    #*********************** END ADDED BY SHANE ***********************      \n",
    "    for y in years:\n",
    "        wit_yearly = wit_df[pd.DatetimeIndex(wit_df['date']).year==y][[pkey, 'bs']].groupby(pkey).count()\n",
    "        wit_yearly.insert(0, 'year', y)\n",
    "        wit_yearly = wit_yearly.rename(columns={n: 'count' for n in wit_yearly.columns[1:]})\n",
    "        #shane changed index from 3 to 4 to accomodate median added above \n",
    "        wit_metrics[4] = wit_metrics[4].append(wit_yearly, sort=False)\n",
    "    #for t in threshold:\n",
    "    #    wit_df_ts = wit_df.copy(deep=True)\n",
    "    #    wit_metrics += [pd.DataFrame()]\n",
    "    #    wit_df_ts.loc[:, wit_df_ts.columns[2:]] = wit_df_ts.loc[:, wit_df_ts.columns[2:]].mask((wit_df_ts[wit_df_ts.columns[2:]] < t/100), np.nan)\n",
    "    #    for y in years:\n",
    "    #        wit_yearly = wit_df_ts[pd.DatetimeIndex(wit_df_ts['date']).year==y].drop(columns=['date']).groupby(pkey).count()\n",
    "    #        wit_yearly.insert(0, 'year', y)\n",
    "    #        wit_yearly = wit_yearly.rename(columns={n: n+'_count'+str(t) for n in wit_yearly.columns[1:]})\n",
    "    #        wit_metrics[-1] = wit_metrics[-1].append(wit_yearly, sort=False)\n",
    "    wit_yearly_metrics = wit_metrics[0]\n",
    "    wit_yearly_metrics.sort_values(by=[pkey, 'year'],inplace=True)\n",
    "    for i in range(len(wit_metrics)-1):\n",
    "        wit_yearly_metrics = pd.merge(wit_yearly_metrics, wit_metrics[i+1], on=[pkey, 'year'], how='inner')\n",
    "    ofn = \"WIT_ANAE_yearly_metrics\"+str(wit_data['chunk'].iat[0])+\".csv\"\n",
    "    wit_yearly_metrics.to_csv(ofn)\n",
    "    return wit_yearly_metrics\n",
    "\n",
    "def get_event_time(wit_ww, threshold, pkey='feature_id'):\n",
    "    \"\"\"\n",
    "\n",
    "        Compute inundation event time by given threshold\n",
    "        input:\n",
    "            wit_df: wetness computed from wit data\n",
    "            threshold: a value such that (water+wet > threshold) = inundation\n",
    "        output:\n",
    "            dateframe of inundation event time\n",
    "    \"\"\"\n",
    "    if isinstance(threshold, pd.DataFrame):\n",
    "        gid = wit_ww.index.unique()[0]\n",
    "        poly_threshold = threshold.loc[gid].to_numpy()[0]\n",
    "    else:\n",
    "        poly_threshold = threshold\n",
    "    i_start = wit_ww[wit_ww['water+wet'] >= poly_threshold]['date'].min()\n",
    "    if pd.isnull(i_start):\n",
    "        re = pd.DataFrame([[np.nan] * 5], columns=['threshold', 'start_time', 'end_time', 'duration', 'gap'], index=wit_ww.index.unique())\n",
    "        re.index.name = pkey\n",
    "        return re\n",
    "    #SSB - moved equal to needed for when threshold = 0\n",
    "    #re_idx = np.searchsorted(wit_ww[(wit_ww['water+wet'] < poly_threshold)]['date'].values, \n",
    "    #                         wit_ww[(wit_ww['water+wet'] >= poly_threshold)]['date'].values)\n",
    "    re_idx = np.searchsorted(wit_ww[(wit_ww['water+wet'] <= poly_threshold)]['date'].values,\n",
    "                         wit_ww[(wit_ww['water+wet'] > poly_threshold)]['date'].values)\n",
    "\n",
    "    re_idx, count = np.unique(re_idx, return_counts=True)\n",
    "    start_idx = np.zeros(len(count)+1, dtype='int')\n",
    "    start_idx[1:] = np.cumsum(count)\n",
    "\n",
    "    #SSB removed \"equals\" sorts correctly when threshold is zero\n",
    "    #re_start = wit_ww[(wit_ww['water+wet'] >= poly_threshold)].iloc[start_idx[:-1]][['date']].rename(columns={'date': 'start_time'})\n",
    "    #re_end = wit_ww[(wit_ww['water+wet'] >= poly_threshold)].iloc[start_idx[1:] - 1][['date']].rename(columns={'date': 'end_time'})\n",
    "    re_start = wit_ww[(wit_ww['water+wet'] > poly_threshold)].iloc[start_idx[:-1]][['date']].rename(columns={'date': 'start_time'})\n",
    "    re_end = wit_ww[(wit_ww['water+wet'] > poly_threshold)].iloc[start_idx[1:] - 1][['date']].rename(columns={'date': 'end_time'})\n",
    "\n",
    "    re = pd.concat([re_start, re_end], axis=1)\n",
    "    if not re.empty:\n",
    "        re.insert(2, 'duration', \n",
    "                  (re['end_time'] - re['start_time'] + np.timedelta64(1, 'D')).astype('timedelta64[D]').astype('timedelta64[D]'))\n",
    "        re.insert(3, 'gap', np.concatenate([[np.timedelta64(0, 'D')],\n",
    "                                            (re['start_time'][1:].values - re['end_time'][:-1].values - np.timedelta64(1, 'D')).astype('timedelta64[D]')]))\n",
    "        re.insert(0, 'threshold', poly_threshold)\n",
    "        re.insert(0, pkey, wit_ww.index.unique()[0])\n",
    "        re = re.set_index(pkey)\n",
    "    return re\n",
    "    \n",
    "def get_im_stats(grouped_wit, im_time, wit_area):\n",
    "    \"\"\"\n",
    "        Get inundation stats given wit data and events\n",
    "        input:\n",
    "            grouped_wit: wit data\n",
    "            im_time: inundation events in time\n",
    "        output:\n",
    "            the stats of inundation events\n",
    "    \"\"\"\n",
    "    gid = grouped_wit.index.unique()[0]\n",
    "    if gid not in im_time.indices.keys():\n",
    "        return pd.DataFrame([[np.nan]*5], columns=['start_time', 'max_water+wet', 'mean_water+wet', 'max_wet_area', 'mean_wet_area'],\n",
    "                           index=[gid])\n",
    "    re_left = np.searchsorted(grouped_wit['date'].values.astype('datetime64'),\n",
    "                         im_time.get_group(gid)['start_time'].values, side='left')\n",
    "    re_right = np.searchsorted(grouped_wit['date'].values.astype('datetime64'),\n",
    "                         im_time.get_group(gid)['end_time'].values, side='right')\n",
    "    re = pd.DataFrame()\n",
    "    for a, b in zip(re_left, re_right):\n",
    "        tmp = pd.concat([grouped_wit.iloc[a:a+1]['date'].rename('start_time').astype('datetime64'),\n",
    "                         pd.Series(grouped_wit.iloc[a:b]['water+wet'].max(),index=[gid], name='max_water+wet'),\n",
    "                         pd.Series(grouped_wit.iloc[a:b]['water+wet'].mean(),index=[gid], name='mean_water+wet')],\n",
    "                        axis=1)\n",
    "        if isinstance(wit_area, pd.DataFrame):\n",
    "            tmp.insert(3, 'max_wet_area', tmp['max_water+wet'].values * wit_area[wit_area.index==gid].values)\n",
    "            tmp.insert(4, 'mean_wet_area', tmp['mean_water+wet'].values * wit_area[wit_area.index==gid].values)\n",
    "        re = re.append(tmp, sort=False)\n",
    "    #reset the index as the pkey\n",
    "    re.index.name = grouped_wit.index.name\n",
    "    re.reset_index()\n",
    "    return re\n",
    "\n",
    "def event_time(wit_df, threshold=0.01, pkey='feature_id'):\n",
    "    \"\"\"\n",
    "        Compute the inundation events with given wit data and threshold\n",
    "        input:\n",
    "            wit_df: wetness computed from wit data\n",
    "            threshold: a value such that (water+wet > threshold) = inundation,\n",
    "        output:\n",
    "            dataframe of events\n",
    "    \"\"\"\n",
    "    return wit_df.groupby(pkey).apply(get_event_time, threshold=threshold, pkey=pkey).dropna().droplevel(0)\n",
    "\n",
    "def event_stats(wit_df, wit_im, wit_area, pkey='feature_id'):\n",
    "    \"\"\"\n",
    "        Compute inundation event stats with given wit wetness, events defined by (start_time, end_time) \n",
    "        and polygon areas\n",
    "        input:\n",
    "            wit_df: wetness computed from wit data\n",
    "            wit_im: inundation event\n",
    "            wit_area: polygon areas indexed by the key\n",
    "        output:\n",
    "            dataframe of event stats\n",
    "    \"\"\"\n",
    "    grouped_im = wit_im[['start_time', 'end_time']].groupby(pkey)\n",
    "    #was droplevel(0) but this left first column without a header (1) deletes that column instead\n",
    "    return wit_df.groupby(pkey).apply(get_im_stats, im_time=grouped_im, wit_area=wit_area).droplevel(1)\n",
    "\n",
    "def inundation_metrics(wit_data, threshold=0.01, shapefile = 'shapefile', skey='UID', pkey='feature_id'):\n",
    "    \"\"\"\n",
    "        Compute inundation metrics with given wit data, polygon areas and threshold\n",
    "        input:\n",
    "            wit_data: a dataframe of wit_data\n",
    "            wit_area: polygon areas indexed by the key\n",
    "            threshold: a value such that (water+wet > threshold) = inundation\n",
    "        output:\n",
    "            dataframe of inundation metrics\n",
    "    \"\"\"\n",
    "    wit_area=[]\n",
    "    if os.path.isfile(shapefile):\n",
    "        features = shape_list(skey, wit_data['feature_id'].unique(), shapefile)\n",
    "        wit_area = get_areas(features, pkey='feature_id')\n",
    "\n",
    "    wit_df = wit_data.copy(deep=True)\n",
    "    wit_df.insert(2, 'water+wet', wit_df[['water', 'wet']].sum(axis=1).round(decimals = 4))\n",
    "    #wit_df = wit_df.drop(columns=wit_df.columns[3:])\n",
    "    wit_df = wit_df[[pkey,'date','water+wet']]\n",
    "    wit_df['date'] = wit_df['date'].astype('datetime64')\n",
    "    wit_df = wit_df.set_index(pkey)\n",
    "    wit_im_time = event_time(wit_df, threshold, pkey='feature_id')\n",
    "    ofn = \"WIT_ANAE_event_times\"+str(wit_data['chunk'].iat[0])+\".csv\"\n",
    "    wit_im_time.to_csv(ofn)\n",
    "    \n",
    "    wit_im_stats = event_stats(wit_df, wit_im_time, wit_area, pkey='feature_id')\n",
    "    ofn = \"WIT_ANAE_event_stats\"+str(wit_data['chunk'].iat[0])+\".csv\"\n",
    "    wit_im_stats.to_csv(ofn)\n",
    "    \n",
    "    wit_im = pd.DataFrame()\n",
    "    if not wit_im_time.empty:\n",
    "        wit_im =pd.merge(wit_im_time, wit_im_stats, on=[pkey, 'start_time'], how='inner')\n",
    "        ofn = \"WIT_ANAE_inundation_metrics\"+str(wit_data['chunk'].iat[0])+\".csv\"\n",
    "        wit_im.to_csv(ofn)\n",
    "    return wit_im\n",
    "\n",
    "def interpolate_daily(wit_data, pkey='feature_id'):\n",
    "    return wit_data.groupby(pkey).apply(interpolate_wit, pkey=pkey).droplevel(0)\n",
    "\n",
    "def interpolate_wit(grouped_wit, pkey='feature_id'):\n",
    "    daily_wit = pd.DataFrame({pkey: grouped_wit[pkey].unique()[0], 'date': pd.date_range(grouped_wit['date'].astype('datetime64[D]').min(), grouped_wit['date'].astype('datetime64[D]').max(), freq='D'),\n",
    "                          'bs': np.nan, 'npv': np.nan, 'pv': np.nan, 'wet': np.nan, 'water': np.nan})\n",
    "    _, nidx, oidx = np.intersect1d(daily_wit['date'].to_numpy().astype('datetime64[D]'), grouped_wit['date'].to_numpy().astype('datetime64[D]'),\n",
    "                  return_indices=True)\n",
    "    daily_wit.loc[nidx, [\"bs\",\"npv\",\"pv\",\"wet\",\"water\"]]  = grouped_wit[[\"bs\",\"npv\",\"pv\",\"wet\",\"water\"]].iloc[oidx].to_numpy()\n",
    "    #daily_wit = daily_wit.interpolate(axis=0)\n",
    "    #recent version of pandas throws error due to date column.  workaround is to only interpolate the columns of data\n",
    "    daily_wit[[\"bs\",\"npv\",\"pv\",\"wet\",\"water\"]] = daily_wit.groupby(['feature_id']).apply(lambda x: x[[\"bs\",\"npv\",\"pv\",\"wet\",\"water\"]].interpolate(axis=0))\n",
    "    if 'chunk' in grouped_wit.columns:\n",
    "        daily_wit['chunk'] = grouped_wit['chunk'].unique()[0]\n",
    "    return daily_wit\n",
    "\n",
    "def time_since_last_inundation(wit_data, wit_im, pkey='feature_id'):\n",
    "    \"\"\"\n",
    "        create a pivot table to gather the time since last inundation using the event metrics\n",
    "        timesincelast = number of days from last event end-date to final date in WIT record\n",
    "    \"\"\"\n",
    "    maxdate = pd.pivot_table(wit_data, index=pkey, values=['date'], aggfunc=np.max)\n",
    "    maxdate['final-date'] = maxdate['date'].astype('datetime64')\n",
    "    lastevent = pd.pivot_table(wit_im, index=pkey, values=['end_time'], aggfunc=np.max)\n",
    "    time_since_last = pd.merge(maxdate, lastevent, on=[pkey], how='inner')\n",
    "    time_since_last.insert(2, 'timesincelast', \n",
    "              (time_since_last['final-date'] - time_since_last['end_time']).astype('timedelta64[D]'))\n",
    "    ofn = \"WIT_ANAE_time_since_last_inundation\"+str(wit_data['chunk'].iat[0])+\".csv\"\n",
    "    time_since_last.to_csv(ofn)\n",
    "    return time_since_last\n",
    "\n",
    "def all_time_median(wit_data, members=[['water', 'wet']], pkey='feature_id'):\n",
    "    \"\"\"\n",
    "        Compute the all time median\n",
    "        input:\n",
    "            wit_data: dataframe of WIT\n",
    "            members: the elements which the metrics are computed against, can be a column from wit_data, e.g. 'pv'\n",
    "                         or the sum of wit columns, e.g. ['water', 'wet']\n",
    "        output:\n",
    "            dataframe of median indexed by pkey\n",
    "    \"\"\"\n",
    "    wit_df = wit_data.copy(deep=True)\n",
    "    i = 0\n",
    "    for m in members:\n",
    "        if isinstance(m, list):\n",
    "            wit_df.insert(wit_df.columns.size+i, '+'.join(m), wit_df[m].sum(axis=1))\n",
    "        i += 1\n",
    "    wit_median = wit_df.groupby(pkey).median().round(decimals = 4)\n",
    "    ofn = \"WIT_ANAE_event_threshold\"+str(wit_data['chunk'].iat[0])+\".csv\"\n",
    "    wit_median.to_csv(ofn)\n",
    "    return wit_median\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages\n",
    "Import Python packages that are used for the analysis.\n",
    "\n",
    "Use standard import commands; some are shown below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "#import os  # is loaded above to set the working directory\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fiona\n",
    "import glob\n",
    "from shapely import geometry\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import multiprocessing\n",
    "from itertools import repeat\n",
    "\n",
    "sys.path.append(os.getcwd())   #appends cwd to path allowing python to find the workers.py in the notebook directory\n",
    "import workers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitons\n",
    "* **Note:**\n",
    "   * Many of the functions defined below are duplicates of the code contained in the workers module for use when debugging or when running on single core\n",
    "   * The main code below uses multiprocessing to call worker functions in workers.py that is located in the current working directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Jupyter can switch this code off to get it out of your way when running multiprocessor\n",
    "    esc y to activate code so it can run\n",
    "    esc r to switch to raw and disable the cell\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def shape_list(key, values, shapefile):\n",
    "    \"\"\"\n",
    "        Get a generator of shapes from the given shapefile\n",
    "            key: the key to match in 'properties' in the shape file\n",
    "            values: a list of property values\n",
    "            shapefile: the name of your shape file\n",
    "            e.g. key='ORIGID', values=[1, 2, 3, 4, 5], \n",
    "            shapefile='/g/data/r78/DEA_Wetlands/shapefiles/MDB_ANAE_Aug2017_modified_2019_SB_3577.shp'\n",
    "    \"\"\"\n",
    "    count = len(values)\n",
    "    with fiona.open(shapefile) as allshapes:\n",
    "        for shape in allshapes:\n",
    "            shape_id = shape['properties'].get(key)\n",
    "            if shape_id is None:\n",
    "                continue\n",
    "            if shape_id in values:\n",
    "                yield(shape_id, shape)\n",
    "                count -= 1\n",
    "            if count <= 0:\n",
    "                break\n",
    "    \n",
    "def get_areas(features, pkey='feature_id'):\n",
    "    \"\"\"\n",
    "        Calculate the area of a list/generator of shapes\n",
    "        input:\n",
    "            features: a list of shapes indexed by the key\n",
    "        output:\n",
    "            a dataframe of area index by the key\n",
    "    \"\"\"\n",
    "    re = pd.DataFrame()\n",
    "    for f in features:\n",
    "        va = pd.DataFrame([[f[0], geometry.shape(f[1]['geometry']).area/1e4]], columns=[pkey, 'area'])\n",
    "        re = re.append(va, sort=False)\n",
    "    return re.set_index(pkey)\n",
    "\n",
    "def dump_wit_data(key, feature_list, output):\n",
    "    \"\"\"\n",
    "        dump wit data from the database into a file\n",
    "        input:\n",
    "            key: Name to id the polygon\n",
    "            feature_list: a list or generator of features\n",
    "        output:\n",
    "            a csv file to save all the wit data\n",
    "    \"\"\"\n",
    "    for f_id, f in feature_list:\n",
    "        _, wit_data = query_wit_data(f)\n",
    "        csv_buf = io.StringIO()\n",
    "        wit_df = pd.DataFrame(data=wit_data, columns=['date', 'bs', 'npv', 'pv', 'wet', 'water'])\n",
    "        wit_df.insert(0, key, f_id)\n",
    "        wit_df.to_csv(csv_buf, index=False, header=False)\n",
    "        csv_buf.seek(0)\n",
    "        with open(output, 'a') as f:\n",
    "            f.write(csv_buf.read())\n",
    "    with open(output, 'a') as f:\n",
    "        f.write(','.join(list(wit_df.columns))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    esc y to activate code so it can run\n",
    "    esc r to switch to raw and disable the cell\n",
    "\"\"\"\n",
    "def annual_metrics(wit_data, members=['pv', 'wet', 'water', 'bs', 'npv', ['npv', 'pv', 'wet'],\n",
    "                                          ['pv', 'wet'], ['water', 'wet']], threshold=[25, 75], pkey='feature_id'):\n",
    "    \"\"\"\n",
    "        Compute the annual max, min, mean, count with given wit data, members and threshold\n",
    "        input:\n",
    "            wit_data: dataframe of WIT\n",
    "            members: the elements which the metrics are computed against, can be a column from wit_data, e.g. 'pv'\n",
    "                         or the sum of wit columns, e.g. ['water', 'wet']\n",
    "            threshold: a list of thresholds such that (elements >= threshold[i]) is True, \n",
    "                        where i = 0, 1...len(threshold)-1\n",
    "        output:\n",
    "            dataframe of metrics\n",
    "    \"\"\"\n",
    "    years = wit_data['date']\n",
    "    i = 0\n",
    "    wit_df = wit_data.copy(deep=True)\n",
    "    for m in members:\n",
    "        if isinstance(m, list):\n",
    "            wit_df.insert(wit_df.columns.size+i, '+'.join(m), wit_df[m].sum(axis=1))\n",
    "    years = pd.DatetimeIndex(wit_df['date']).year.unique()\n",
    "    shape_id_list = wit_df[pkey].unique()\n",
    "    #shane changed 4 to 5 to accomodate median added below \n",
    "    wit_metrics = [pd.DataFrame()] * 5\n",
    "    for y in years:\n",
    "        wit_yearly = wit_df[pd.DatetimeIndex(wit_df['date']).year==y].drop(columns=['date']).groupby(pkey).max()\n",
    "        wit_yearly.insert(0, 'year', y)\n",
    "        wit_yearly = wit_yearly.rename(columns={n: n+'_max' for n in wit_yearly.columns[1:]})\n",
    "        wit_metrics[0] = wit_metrics[0].append(wit_yearly, sort=False)\n",
    "    for y in years:\n",
    "        wit_yearly = wit_df[pd.DatetimeIndex(wit_df['date']).year==y].drop(columns=['date']).groupby(pkey).min()\n",
    "        wit_yearly.insert(0, 'year', y)\n",
    "        wit_yearly = wit_yearly.rename(columns={n: n+'_min' for n in wit_yearly.columns[1:]})\n",
    "        wit_metrics[1] = wit_metrics[1].append(wit_yearly, sort=False)\n",
    "    for y in years:\n",
    "        wit_yearly = wit_df[pd.DatetimeIndex(wit_df['date']).year==y].drop(columns=['date']).groupby(pkey).mean()\n",
    "        wit_yearly.insert(0, 'year', y)\n",
    "        wit_yearly = wit_yearly.rename(columns={n: n+'_mean' for n in wit_yearly.columns[1:]})\n",
    "        wit_metrics[2] = wit_metrics[2].append(wit_yearly, sort=False)\n",
    "\n",
    "    #*********************** START ADDED BY SHANE ***********************\n",
    "    #adding median\n",
    "    for y in years:\n",
    "        wit_yearly = wit_df[pd.DatetimeIndex(wit_df['date']).year==y].drop(columns=['date']).groupby(pkey).median()\n",
    "        wit_yearly.insert(0, 'year', y)\n",
    "        wit_yearly = wit_yearly.rename(columns={n: n+'_median' for n in wit_yearly.columns[1:]})\n",
    "        wit_metrics[3] = wit_metrics[3].append(wit_yearly, sort=False)\n",
    "    #*********************** END ADDED BY SHANE ***********************      \n",
    "    for y in years:\n",
    "        wit_yearly = wit_df[pd.DatetimeIndex(wit_df['date']).year==y][[pkey, 'bs']].groupby(pkey).count()\n",
    "        wit_yearly.insert(0, 'year', y)\n",
    "        wit_yearly = wit_yearly.rename(columns={n: 'count' for n in wit_yearly.columns[1:]})\n",
    "        #shane changed index from 3 to 4 to accomodate median added above \n",
    "        wit_metrics[4] = wit_metrics[4].append(wit_yearly, sort=False)\n",
    "    #for t in threshold:\n",
    "    #    wit_df_ts = wit_df.copy(deep=True)\n",
    "    #    wit_metrics += [pd.DataFrame()]\n",
    "    #    wit_df_ts.loc[:, wit_df_ts.columns[2:]] = wit_df_ts.loc[:, wit_df_ts.columns[2:]].mask((wit_df_ts[wit_df_ts.columns[2:]] < t/100), np.nan)\n",
    "    #    for y in years:\n",
    "    #        wit_yearly = wit_df_ts[pd.DatetimeIndex(wit_df_ts['date']).year==y].drop(columns=['date']).groupby(pkey).count()\n",
    "    #        wit_yearly.insert(0, 'year', y)\n",
    "    #        wit_yearly = wit_yearly.rename(columns={n: n+'_count'+str(t) for n in wit_yearly.columns[1:]})\n",
    "    #        wit_metrics[-1] = wit_metrics[-1].append(wit_yearly, sort=False)\n",
    "    wit_yearly_metrics = wit_metrics[0]\n",
    "    wit_yearly_metrics.sort_values(by=[pkey, 'year'],inplace=True)\n",
    "    for i in range(len(wit_metrics)-1):\n",
    "        wit_yearly_metrics = pd.merge(wit_yearly_metrics, wit_metrics[i+1], on=[pkey, 'year'], how='inner')\n",
    "    return wit_yearly_metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    esc y to activate code so it can run\n",
    "    esc r to switch to raw and disable the cell\n",
    "\"\"\"\n",
    "\n",
    "def get_event_time(wit_ww, threshold, pkey='feature_id'):\n",
    "    \"\"\"\n",
    "\n",
    "        Compute inundation event time by given threshold\n",
    "        input:\n",
    "            wit_df: wetness computed from wit data\n",
    "            threshold: a value such that (water+wet > threshold) = inundation\n",
    "        output:\n",
    "            dateframe of inundation event time\n",
    "    \"\"\"\n",
    "    if isinstance(threshold, pd.DataFrame):\n",
    "        gid = wit_ww.index.unique()[0]\n",
    "        poly_threshold = threshold.loc[gid].to_numpy()[0]\n",
    "    else:\n",
    "        poly_threshold = threshold\n",
    "    i_start = wit_ww[wit_ww['water+wet'] >= poly_threshold]['date'].min()\n",
    "    if pd.isnull(i_start):\n",
    "        re = pd.DataFrame([[np.nan] * 5], columns=['threshold', 'start_time', 'end_time', 'duration', 'gap'], index=wit_ww.index.unique())\n",
    "        re.index.name = pkey\n",
    "        return re\n",
    "    #SSB - moved equal to needed for when threshold = 0\n",
    "    #re_idx = np.searchsorted(wit_ww[(wit_ww['water+wet'] < poly_threshold)]['date'].values, \n",
    "    #                         wit_ww[(wit_ww['water+wet'] >= poly_threshold)]['date'].values)\n",
    "    re_idx = np.searchsorted(wit_ww[(wit_ww['water+wet'] <= poly_threshold)]['date'].values,\n",
    "                         wit_ww[(wit_ww['water+wet'] > poly_threshold)]['date'].values)\n",
    "\n",
    "    re_idx, count = np.unique(re_idx, return_counts=True)\n",
    "    start_idx = np.zeros(len(count)+1, dtype='int')\n",
    "    start_idx[1:] = np.cumsum(count)\n",
    "\n",
    "    #SSB removed \"equals\" sorts correctly when threshold is zero\n",
    "    #re_start = wit_ww[(wit_ww['water+wet'] >= poly_threshold)].iloc[start_idx[:-1]][['date']].rename(columns={'date': 'start_time'})\n",
    "    #re_end = wit_ww[(wit_ww['water+wet'] >= poly_threshold)].iloc[start_idx[1:] - 1][['date']].rename(columns={'date': 'end_time'})\n",
    "    re_start = wit_ww[(wit_ww['water+wet'] > poly_threshold)].iloc[start_idx[:-1]][['date']].rename(columns={'date': 'start_time'})\n",
    "    re_end = wit_ww[(wit_ww['water+wet'] > poly_threshold)].iloc[start_idx[1:] - 1][['date']].rename(columns={'date': 'end_time'})\n",
    "\n",
    "    re = pd.concat([re_start, re_end], axis=1)\n",
    "    if not re.empty:\n",
    "        re.insert(2, 'duration', \n",
    "                  (re['end_time'] - re['start_time'] + np.timedelta64(1, 'D')).astype('timedelta64[D]').astype('timedelta64[D]'))\n",
    "        re.insert(3, 'gap', np.concatenate([[np.timedelta64(0, 'D')],\n",
    "                                            (re['start_time'][1:].values - re['end_time'][:-1].values - np.timedelta64(1, 'D')).astype('timedelta64[D]')]))\n",
    "        re.insert(0, 'threshold', poly_threshold)\n",
    "        re.insert(0, pkey, wit_ww.index.unique()[0])\n",
    "        re = re.set_index(pkey)\n",
    "    return re\n",
    "    \n",
    "def get_im_stats(grouped_wit, im_time, wit_area):\n",
    "    \"\"\"\n",
    "        Get inundation stats given wit data and events\n",
    "        input:\n",
    "            grouped_wit: wit data\n",
    "            im_time: inundation events in time\n",
    "        output:\n",
    "            the stats of inundation events\n",
    "    \"\"\"\n",
    "    gid = grouped_wit.index.unique()[0]\n",
    "    if gid not in im_time.indices.keys():\n",
    "        return pd.DataFrame([[np.nan]*5], columns=['start_time', 'max_water+wet', 'mean_water+wet', 'max_wet_area', 'mean_wet_area'],\n",
    "                           index=[gid])\n",
    "    re_left = np.searchsorted(grouped_wit['date'].values.astype('datetime64'),\n",
    "                         im_time.get_group(gid)['start_time'].values, side='left')\n",
    "    re_right = np.searchsorted(grouped_wit['date'].values.astype('datetime64'),\n",
    "                         im_time.get_group(gid)['end_time'].values, side='right')\n",
    "    re = pd.DataFrame()\n",
    "    for a, b in zip(re_left, re_right):\n",
    "        tmp = pd.concat([grouped_wit.iloc[a:a+1]['date'].rename('start_time').astype('datetime64'),\n",
    "                         pd.Series(grouped_wit.iloc[a:b]['water+wet'].max(),index=[gid], name='max_water+wet'),\n",
    "                         pd.Series(grouped_wit.iloc[a:b]['water+wet'].mean(),index=[gid], name='mean_water+wet')],\n",
    "                        axis=1)\n",
    "        if isinstance(wit_area, pd.DataFrame):\n",
    "            tmp.insert(3, 'max_wet_area', tmp['max_water+wet'].values * wit_area[wit_area.index==gid].values)\n",
    "            tmp.insert(4, 'mean_wet_area', tmp['mean_water+wet'].values * wit_area[wit_area.index==gid].values)\n",
    "        \n",
    "        re = re.append(tmp, sort=False)\n",
    "    #reset the index as the pkey\n",
    "    re.index.name = grouped_wit.index.name\n",
    "    re.reset_index()\n",
    "    #re.to_csv('ssb.csv')\n",
    "    return re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    esc y to activate code so it can run\n",
    "    esc r to switch to raw and disable the cell\n",
    "\"\"\"\n",
    "def event_time(wit_df, threshold=0.01, pkey='feature_id'):\n",
    "    \"\"\"\n",
    "        Compute the inundation events with given wit data and threshold\n",
    "        input:\n",
    "            wit_df: wetness computed from wit data\n",
    "            threshold: a value such that (water+wet > threshold) = inundation,\n",
    "        output:\n",
    "            dataframe of events\n",
    "    \"\"\"\n",
    "    return wit_df.groupby(pkey).apply(get_event_time, threshold=threshold, pkey=pkey).dropna().droplevel(0)\n",
    "\n",
    "def event_stats(wit_df, wit_im, wit_area, pkey='feature_id'):\n",
    "    \"\"\"\n",
    "        Compute inundation event stats with given wit wetness, events defined by (start_time, end_time) \n",
    "        and polygon areas\n",
    "        input:\n",
    "            wit_df: wetness computed from wit data\n",
    "            wit_im: inundation event\n",
    "            wit_area: polygon areas indexed by the key\n",
    "        output:\n",
    "            dataframe of event stats\n",
    "    \"\"\"\n",
    "    grouped_im = wit_im[['start_time', 'end_time']].groupby(pkey)\n",
    "    #was droplevel(0) but this left first column without a header (1) deletes that column instead\n",
    "    return  wit_df.groupby(pkey).apply(get_im_stats, im_time=grouped_im, wit_area=wit_area).droplevel(1)\n",
    "\n",
    "def inundation_metrics(wit_data, threshold=0.01, shapefile = 'shapefile', skey='UID', pkey='feature_id'):\n",
    "    \"\"\"\n",
    "        Compute inundation metrics with given wit data, polygon areas and threshold\n",
    "        input:\n",
    "            wit_data: a dataframe of wit_data\n",
    "            wit_area: polygon areas indexed by the key\n",
    "            threshold: a value such that (water+wet > threshold) = inundation\n",
    "        output:\n",
    "            dataframe of inundation metrics\n",
    "    \"\"\"\n",
    "    wit_area=[]\n",
    "    if os.path.isfile(shapefile):\n",
    "        features = shape_list(skey, wit_data['feature_id'].unique(), shapefile)\n",
    "        wit_area = get_areas(features, pkey='feature_id')\n",
    "        \n",
    "    wit_df = wit_data.copy(deep=True)\n",
    "    wit_df.insert(2, 'water+wet', wit_df[['water', 'wet']].sum(axis=1).round(4))\n",
    "    #wit_df = wit_df.drop(columns=wit_df.columns[3:])\n",
    "    wit_df = wit_df[[pkey,'date','water+wet']]\n",
    "    wit_df['date'] = wit_df['date'].astype('datetime64')\n",
    "\n",
    "    ofn = \"ANAE_wit_df\"+str(wit_data['chunk'].iat[0])+\".csv\"\n",
    "    wit_df.to_csv(ofn)\n",
    "    wit_df = wit_df.set_index(pkey)\n",
    "    wit_im_time = event_time(wit_df, threshold, pkey)\n",
    "    ofn = \"ANAE_event_times\"+str(wit_data['chunk'].iat[0])+\".csv\"\n",
    "    wit_im_time.to_csv(ofn)\n",
    "    wit_im_stats = event_stats(wit_df, wit_im_time, wit_area, pkey)\n",
    "    if not wit_im_time.empty:\n",
    "        ofn = \"ANAE_event_stats\"+str(wit_data['chunk'].iat[0])+\".csv\"\n",
    "        wit_im_stats.to_csv(ofn)\n",
    "\n",
    "    wit_im = pd.DataFrame()\n",
    "    if not wit_im_time.empty:\n",
    "        wit_im =pd.merge(wit_im_time, wit_im_stats, on=[pkey, 'start_time'], how='inner')\n",
    "        ofn = \"ANAE_inundation_metrics\"+str(wit_data['chunk'].iat[0])+\".csv\"\n",
    "        wit_im.to_csv(ofn)\n",
    "\n",
    "    return wit_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    esc y to activate code so it can run\n",
    "    esc r to switch to raw and disable the cell\n",
    "\"\"\"\n",
    "\n",
    "def interpolate_wit(grouped_wit, pkey='feature_id'):\n",
    "    daily_wit = pd.DataFrame({pkey: grouped_wit[pkey].unique()[0], 'date': pd.date_range(grouped_wit['date'].astype('datetime64[D]').min(), grouped_wit['date'].astype('datetime64[D]').max(), freq='D'),\n",
    "                          'bs': np.nan, 'npv': np.nan, 'pv': np.nan, 'wet': np.nan, 'water': np.nan})\n",
    "    _, nidx, oidx = np.intersect1d(daily_wit['date'].to_numpy().astype('datetime64[D]'), grouped_wit['date'].to_numpy().astype('datetime64[D]'),\n",
    "                  return_indices=True)\n",
    "    daily_wit.loc[nidx, [\"bs\",\"npv\",\"pv\",\"wet\",\"water\"]]  = grouped_wit[[\"bs\",\"npv\",\"pv\",\"wet\",\"water\"]].iloc[oidx].to_numpy()\n",
    "    #daily_wit = daily_wit.interpolate(axis=0)\n",
    "    #changes to pandas can't interpolate the date column - can only interpolate the columns of data\n",
    "    daily_wit[[\"bs\",\"npv\",\"pv\",\"wet\",\"water\"]] = daily_wit.groupby(['feature_id']).apply(lambda x: x[[\"bs\",\"npv\",\"pv\",\"wet\",\"water\"]].interpolate(axis=0))\n",
    "    return daily_wit\n",
    "\n",
    "def time_since_last_inundation(wit_data, wit_im, pkey='feature_id'):\n",
    "    \"\"\"\n",
    "        create a pivot table to gather the time since last inundation using the event metrics\n",
    "        timesincelast = number of days from last event end-date to final date in WIT record\n",
    "    \"\"\"\n",
    "    maxdate = pd.pivot_table(wit_data, index=pkey, values=['date'], aggfunc=np.max)\n",
    "    maxdate['final-date'] = maxdate['date'].astype('datetime64')\n",
    "    lastevent = pd.pivot_table(wit_im, index=pkey, values=['end_time'], aggfunc=np.max)\n",
    "    time_since_last = pd.merge(maxdate, lastevent, on=[pkey], how='inner')\n",
    "    time_since_last.insert(2, 'timesincelast', \n",
    "              (time_since_last['final-date'] - time_since_last['end_time']).astype('timedelta64[D]'))\n",
    "    return time_since_last\n",
    "\n",
    "def all_time_median(wit_data, members=[['water', 'wet']], pkey='feature_id'):\n",
    "    \"\"\"\n",
    "        Compute the all time median\n",
    "        input:\n",
    "            wit_data: dataframe of WIT\n",
    "            members: the elements which the metrics are computed against, can be a column from wit_data, e.g. 'pv'\n",
    "                         or the sum of wit columns, e.g. ['water', 'wet']\n",
    "        output:\n",
    "            dataframe of median indexed by pkey\n",
    "    \"\"\"\n",
    "    wit_df = wit_data.copy(deep=True)\n",
    "    i = 0\n",
    "    for m in members:\n",
    "        if isinstance(m, list):\n",
    "            wit_df.insert(wit_df.columns.size+i, '+'.join(m), wit_df[m].sum(axis=1))\n",
    "        i += 1\n",
    "    wit_median = wit_df.groupby(pkey).median().round(decimals = 4)\n",
    "    ofn = \"ANAE_event_threshold\"+str(wit_data['chunk'].iat[0])+\".csv\"\n",
    "    wit_median.to_csv(ofn)\n",
    "    return (wit_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_batches (path, output_filenames):\n",
    "    '''\n",
    "        merges the outputs from each batch into a single file of results for all ANAE polygons\n",
    "    '''\n",
    "    for fname in output_filenames:\n",
    "        ofns = glob.glob(os.path.join(path, fname+'*.csv'))\n",
    "        dfs = []\n",
    "        for ofn in ofns:\n",
    "            try:\n",
    "                df = pd.read_csv(ofn)\n",
    "                dfs.append(df)\n",
    "            except:\n",
    "                print(\"Error reading file: \", ofn)\n",
    "        if dfs:\n",
    "            out_data = pd.concat(dfs)\n",
    "            out_data.reset_index(drop=True,inplace=True)\n",
    "            ofn = \"RESULT_\"+fname+\".csv\"\n",
    "            try:\n",
    "                out_data.to_csv(os.path.join(path, ofn))\n",
    "            except:\n",
    "                print(\"Error writing merged file: \", os.path.join(path, ofn))\n",
    "\n",
    "\n",
    "def delete_old_batch_outputs (path, output_filenames):\n",
    "    '''\n",
    "        deletes the individual batch results which are no longer required after they have been merged\n",
    "    '''\n",
    "    for fname in output_filenames:\n",
    "        ofns = glob.glob(os.path.join(path, './'+fname+'*.csv'))\n",
    "        for ofn in ofns:\n",
    "            try:\n",
    "                os.remove(ofn)\n",
    "            except:\n",
    "                print(\"Error while deleting file : \", ofn)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multiprocessing code  -  this is the loop that executes to compute the metrics for all CSV in the specified path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37 WIT csv in s:/luke/*.csv\n",
      "Processing using 31 cores, and batches of 200 files.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638118aa32214311abbdc5b68fdb76a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 All batches completed in 00 hours 00 minutes 00 seconds\n",
      "All batches merged in 00 hours 00 minutes 01 seconds\n"
     ]
    }
   ],
   "source": [
    "#note is some debug code commented out in various places below\n",
    "\n",
    "multiprocessing.set_executable(os.path.join(sys.exec_prefix, 'pythonw.exe'))\n",
    "\n",
    "output_filenames = ['WIT_ANAE_yearly_metrics','WIT_ANAE_event_threshold','WIT_ANAE_inundation_metrics','WIT_ANAE_time_since_last_inundation','WIT_ANAE_event_times','WIT_ANAE_event_stats']\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "\n",
    "def nicetime(s):\n",
    "    return time.strftime(\"%H hours %M minutes %S seconds\", time.gmtime(s))\n",
    "\n",
    "# use glob to get all the csv files \n",
    "# in the folder\n",
    "cwdpath = os.getcwd()\n",
    "\n",
    "\n",
    "delete_old_batch_outputs(cwdpath, output_filenames)\n",
    "\n",
    "if interpolate:\n",
    "    interpath = os.path.join(os.path.dirname(csvfiles),'interpolated')\n",
    "    try:\n",
    "        if not os.path.isdir(interpath): os.mkdir(interpath)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "csv_files = glob.glob(csvfiles)\n",
    "\n",
    "if __name__ ==  '__main__': \n",
    "    shape_area = False\n",
    "    debug = False\n",
    "    pkey = 'feature_id'\n",
    "    \n",
    "    # cpu core count -1 to leave the os a crumb to work with\n",
    "    CPU = multiprocessing.cpu_count() - 1\n",
    "    chunksize = batchsize * CPU\n",
    "    #l_index, r_index = 0, batchsize\n",
    "    #batch = csv_files[l_index:r_index]\n",
    "    print (\"Found\",len(csv_files),\"WIT csv in\",csvfiles)\n",
    "    print (\"Processing using\",CPU,\"cores, and batches of\",batchsize,\"files.\")\n",
    "    pool=multiprocessing.Pool(processes = CPU)\n",
    "    try:\n",
    "        for j in tqdm(range(0, len(csv_files), chunksize)):\n",
    "            mpbatch = csv_files[j:j + chunksize]\n",
    "            wd = []\n",
    "            for i in range(0, len(mpbatch), batchsize):\n",
    "                batch = mpbatch[i:i + batchsize]\n",
    "                #print(i, batch)\n",
    "                startbatch = time.process_time()\n",
    "                dfs = []\n",
    "                UIDs = []\n",
    "                for f in batch:\n",
    "                    df = pd.read_csv(f)\n",
    "                    df = df[df['pc_missing'] < 0.1]\n",
    "                    df = df.sort_values(by=['date'])\n",
    "                    dfs.append(df)\n",
    "                wit_data = pd.concat(dfs)\n",
    "                wit_data['chunk'] = j+i\n",
    "                wd.append(wit_data)\n",
    "                #print (\"Batch\",j+i,\"completed in\", time.process_time() - startbatch, \"seconds.\")\n",
    "            \n",
    "            #print (j,\"**** send to pool *********\")\n",
    "\n",
    "            if interpolate:\n",
    "                wd = pool.map(workers.interpolate_daily, wd)\n",
    "                #debug code below writes out the interpolated data for inspection/saving\n",
    "                #ofn = os.path.join(interpath,\"ANAE_interpolated\"+str(j)+\".csv\")\n",
    "                #print (\"saving interpolated data: \"+ ofn)\n",
    "                #wd[0].to_csv(ofn)\n",
    "            #Annual metrics  min, max, median for all WIT params per year.  Also 'water+wet'\n",
    "            output = pool.map(workers.annual_metrics,wd)\n",
    "            \n",
    "            #Calc threshold for inundation events as the all-time median 'water+wet' \n",
    "            mmedian = pool.map(workers.all_time_median,wd)\n",
    "            threshold_list=[]\n",
    "            for e in mmedian:\n",
    "                threshold_list.append(pd.DataFrame(e['water+wet']))\n",
    "                \n",
    "            #Inundation metrics - applying threshold to 'water+wet'   \n",
    "            wit_im = pool.starmap(workers.inundation_metrics, zip(wd, threshold_list, repeat(shapefile), repeat(skey)))\n",
    "            \n",
    "            #Time since last inundation\n",
    "            time_since_last = pool.starmap(workers.time_since_last_inundation, zip(wd, wit_im))\n",
    "            \n",
    "\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    finally:\n",
    "        pool.close()\n",
    "        pool.join()       \n",
    " \n",
    "    print (len(csv_files),\"All batches completed in\", nicetime(time.process_time() - start))\n",
    "    \n",
    "    merge_batches (cwdpath, output_filenames)\n",
    "    print (\"All batches merged in\", nicetime(time.process_time() - start))\n",
    "    delete_old_batch_outputs (cwdpath, output_filenames)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug Code: Single core processing using functions in this notebook (not calling the external workers)\n",
    "\n",
    "* this is slower of course and will chunk through each CSV one by one but may be easier to debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    esc y to activate code so it can run\n",
    "    esc r to switch to raw and disable the cell\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "debug = False            \n",
    "if debug:\n",
    "\n",
    "    output_filenames = ['WIT_ANAE_yearly_metrics','WIT_ANAE_event_threshold','WIT_ANAE_inundation_metrics','WIT_ANAE_time_since_last_inundation','WIT_ANAE_event_times','WIT_ANAE_event_stats']\n",
    "\n",
    "\n",
    "    start = time.process_time()\n",
    "    # use glob to get all the csv files \n",
    "    # in the folder\n",
    "    cwdpath = os.getcwd()\n",
    "\n",
    "    delete_old_batch_outputs (cwdpath, output_filenames)\n",
    "\n",
    "\n",
    "    #csv_files = glob.glob(os.path.join(path, './test_data/*.csv'))\n",
    "    csv_files = glob.glob('s:/ANAE_WIT_result3/*.csv')\n",
    "    # loop over the list of csv files\n",
    "\n",
    "    shapefile ='' #disables area calcs... faster\n",
    "\n",
    "    shape_area = False\n",
    "    debug = True\n",
    "    pkey = 'feature_id'\n",
    "    batchsize = 1000\n",
    "    l_index, r_index = 0, batchsize\n",
    "    batch = csv_files[l_index:r_index]\n",
    "    while batch:\n",
    "        startbatch = time.process_time()\n",
    "        dfs = []\n",
    "        for f in batch:\n",
    "            df = pd.read_csv(f)\n",
    "            df = df[df['pc_missing'] < 0.5]\n",
    "            df = df.sort_values(by=['date'])\n",
    "            dfs.append(df)\n",
    "        wit_data = pd.concat(dfs)\n",
    "        wit_data['chunk'] = l_index\n",
    "        #wd.append(wit_data)\n",
    "        ##if shape_area:\n",
    "        #    features = shape_list('UID', UIDs, shapefile)    \n",
    "        #    wit_area = get_areas(features, pkey='UID')\n",
    "\n",
    "        if debug: wit_data.to_csv(\"'wit_data\"+str(l_index)+\".csv\")\n",
    "        #wit_data = wit_data.groupby(pkey).apply(interpolate_wit, pkey=pkey).droplevel(0)\n",
    "\n",
    "        wit_data.sort_values(by=['feature_id', 'date'],inplace=True)\n",
    "        wit_data.reset_index(drop=True,inplace=True)\n",
    "        if debug: wit_data.to_csv(\"'wit_data_interp\"+str(l_index)+\".csv\")\n",
    "\n",
    "\n",
    "        # compute yearly metrics and save the results to a csv\n",
    "        # set the output file to your own path\n",
    "        wit_yearly_metrics = annual_metrics(wit_data, pkey=pkey)\n",
    "        ofn = \"ANAE_yearly_metrics\"+str(l_index)+\".csv\"\n",
    "        wit_yearly_metrics.to_csv(ofn)\n",
    "\n",
    "        # compute all time median, serve as the threshold\n",
    "        # save the threshold in a csv, set the output file to your own path\n",
    "        wit_median = all_time_median(wit_data, pkey=pkey)\n",
    "        threshold_list = (wit_median[['water+wet']])\n",
    "        ofn = \"ANAE_event_threshold\"+str(l_index)+\".csv\"\n",
    "        threshold_list.to_csv(ofn)\n",
    "\n",
    "        # compute event metrics with threshold_list then save the results to a csv\n",
    "        # set the output file to your own path\n",
    "        wit_im =inundation_metrics(wit_data, threshold_list, shapefile, pkey=pkey)\n",
    "        ofn = \"ANAE_inundation_metrics\"+str(l_index)+\".csv\"\n",
    "        wit_im.to_csv(ofn)\n",
    "\n",
    "        # compute time since the last inundation event then save the results to a csv\n",
    "        time_since_last = time_since_last_inundation(wit_data, wit_im, pkey)\n",
    "        ofn = \"ANAE_time_since_last_inundation\"+str(l_index)+\".csv\"\n",
    "        time_since_last.to_csv(ofn)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "        print (\"Batch\",l_index,\"completed in\", time.process_time() - startbatch, \"seconds.\")\n",
    "        l_index, r_index = r_index, r_index + batchsize\n",
    "        batch = csv_files[l_index:r_index]\n",
    "\n",
    "\n",
    "\n",
    "    merge_batches (cwdpath, output_filenames)\n",
    "    print (\"All batches completed in\", time.process_time() - start, \"seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
